{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Documents/GitHub/Julia-for-SciML/hands-on/lott`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 50 iterations: 3.2919945635723753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 100 iterations: 1.7058558916650455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 150 iterations: 1.6697049368588788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 200 iterations: 1.6423084245679131\n",
      "Training loss after 201 iterations: 1.6423084245679131\n",
      "  0.002486 seconds (917 allocations: 56.104 KiB, 97.19% compilation time)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 250 iterations: 0.023661287470332845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 300 iterations: 0.013746817618177573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 350 iterations: 0.0032781193503113072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 400 iterations: 0.0017736994450182483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 450 iterations: 0.0016452061747373918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 500 iterations: 0.001415755561505311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 550 iterations: 0.001226557886040906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 600 iterations: 0.001107089612914693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 650 iterations: 0.0009982934123468784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 700 iterations: 0.0009875749757684356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 750 iterations: 0.0009812594981828098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 800 iterations: 0.0009796658875397177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 850 iterations: 0.0009793537523280391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 900 iterations: 0.0009772977493860333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 950 iterations: 0.0009743301969273224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 1000 iterations: 0.0009727084653298941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 1050 iterations: 0.000972080483835656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 1100 iterations: 0.0009717534715880698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 1150 iterations: 0.0009656859249757323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 1200 iterations: 0.000962561381268262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 1250 iterations: 0.0009612859817110582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 1300 iterations: 0.0009591475053966475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 1350 iterations: 0.0009582234874165839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 1400 iterations: 0.0009574676566527876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 1450 iterations: 0.0009569122026897299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 1500 iterations: 0.0009567527113130108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 1550 iterations: 0.0009561199657093553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 1600 iterations: 0.0009549884254749657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 1650 iterations: 0.0009541600089060593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 1700 iterations: 0.0009537452602046372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 1750 iterations: 0.0009536569492935126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss after 1800 iterations: 0.0009534426125349452\n"
     ]
    }
   ],
   "source": [
    "## Environment and packages\n",
    "cd(@__DIR__)\n",
    "using Pkg; Pkg.activate(\"lott\"); Pkg.instantiate()\n",
    "\n",
    "\n",
    "using OrdinaryDiffEq\n",
    "using ModelingToolkit\n",
    "using DataDrivenDiffEq\n",
    "using LinearAlgebra, ComponentArrays\n",
    "using Optimization, OptimizationOptimisers, OptimizationOptimJL #OptimizationFlux for ADAM and OptimizationOptimJL for BFGS\n",
    "using DiffEqSensitivity\n",
    "using Lux\n",
    "using Plots\n",
    "gr()\n",
    "using JLD2, FileIO\n",
    "using Statistics\n",
    "# Set a random seed for reproduceable behaviour\n",
    "using Random\n",
    "rng = Random.default_rng()\n",
    "Random.seed!(1234)\n",
    "\n",
    "#### NOTE\n",
    "# Since the recent release of DataDrivenDiffEq v0.6.0 where a complete overhaul of the optimizers took\n",
    "# place, SR3 has been used. Right now, STLSQ performs better and has been changed.\n",
    "\n",
    "# Create a name for saving ( basically a prefix )\n",
    "svname = \"Scenario_1_\"\n",
    "\n",
    "## Data generation\n",
    "function lotka!(du, u, p, t)\n",
    "    α, β, γ, δ = p\n",
    "    du[1] = α*u[1] - β*u[2]*u[1]\n",
    "    du[2] = γ*u[1]*u[2]  - δ*u[2]\n",
    "end\n",
    "\n",
    "# Define the experimental parameter\n",
    "tspan = (0.0,3.0)\n",
    "u0 = [0.44249296,4.6280594]\n",
    "p_ = [1.3, 0.9, 0.8, 1.8]\n",
    "prob = ODEProblem(lotka!, u0,tspan, p_)\n",
    "solution = solve(prob, Vern7(), abstol=1e-12, reltol=1e-12, saveat = 0.1)\n",
    "\n",
    "# Ideal data\n",
    "X = Array(solution)\n",
    "t = solution.t\n",
    "DX = Array(solution(solution.t, Val{1}))\n",
    "\n",
    "full_problem = DataDrivenProblem(X, t = t, DX = DX)\n",
    "\n",
    "# Add noise in terms of the mean\n",
    "x̄ = mean(X, dims = 2)\n",
    "noise_magnitude = 5e-3\n",
    "Xₙ = X .+ (noise_magnitude*x̄) .* randn(eltype(X), size(X))\n",
    "\n",
    "plot(solution, alpha = 0.75, color = :black, label = [\"True Data\" nothing])\n",
    "scatter!(t, transpose(Xₙ), color = :red, label = [\"Noisy Data\" nothing])\n",
    "## Define the network\n",
    "# Gaussian RBF as activation\n",
    "rbf(x) = exp.(-(x.^2))\n",
    "\n",
    "# Multilayer FeedForward\n",
    "U = Lux.Chain(\n",
    "    Lux.Dense(2,5,rbf), Lux.Dense(5,5, rbf), Lux.Dense(5,5, rbf), Lux.Dense(5,2)\n",
    ")\n",
    "# Get the initial parameters and state variables of the model\n",
    "p, st = Lux.setup(rng, U)\n",
    "\n",
    "# Define the hybrid model\n",
    "function ude_dynamics!(du,u, p, t, p_true)\n",
    "    û = U(u, p, st)[1] # Network prediction\n",
    "    du[1] = p_true[1]*u[1] + û[1]\n",
    "    du[2] = -p_true[4]*u[2] + û[2]\n",
    "end\n",
    "\n",
    "# Closure with the known parameter\n",
    "nn_dynamics!(du,u,p,t) = ude_dynamics!(du,u,p,t,p_)\n",
    "# Define the problem (Fix: https://discourse.julialang.org/t/issue-with-ude-repository-lv-scenario-1/88618/5)\n",
    "prob_nn = ODEProblem{true, SciMLBase.FullSpecialize}(nn_dynamics!,Xₙ[:, 1], tspan, p)\n",
    "\n",
    "## Function to train the network\n",
    "# Define a predictor\n",
    "function predict(θ, X = Xₙ[:,1], T = t)\n",
    "    _prob = remake(prob_nn, u0 = X, tspan = (T[1], T[end]), p = θ)\n",
    "    Array(solve(_prob, Vern7(), saveat = T,\n",
    "                abstol=1e-6, reltol=1e-6,\n",
    "                sensealg = ForwardDiffSensitivity()\n",
    "                ))\n",
    "end\n",
    "\n",
    "# Simple L2 loss\n",
    "function loss(θ)\n",
    "    X̂ = predict(θ)\n",
    "    sum(abs2, Xₙ .- X̂)\n",
    "end\n",
    "\n",
    "# Container to track the losses\n",
    "losses = Float64[]\n",
    "\n",
    "callback = function (p, l)\n",
    "  push!(losses, l)\n",
    "  if length(losses)%50==0\n",
    "      println(\"Current loss after $(length(losses)) iterations: $(losses[end])\")\n",
    "  end\n",
    "  return false\n",
    "end\n",
    "\n",
    "## Training\n",
    "\n",
    "# First train with ADAM for better convergence -> move the parameters into a\n",
    "# favourable starting positing for BFGS\n",
    "adtype = Optimization.AutoZygote()\n",
    "optf = Optimization.OptimizationFunction((x,p)->loss(x), adtype)\n",
    "optprob = Optimization.OptimizationProblem(optf, ComponentVector{Float64}(p))\n",
    "res1 = Optimization.solve(optprob, ADAM(0.1), callback=callback, maxiters = 200)\n",
    "println(\"Training loss after $(length(losses)) iterations: $(losses[end])\")\n",
    "# Train with BFGS\n",
    "@time optprob2 = Optimization.OptimizationProblem(optf, res1.minimizer)\n",
    "@time res2 = Optimization.solve(optprob2, Optim.BFGS(initial_stepnorm=0.01), callback=callback, maxiters = 3000)\n",
    "println(\"Final training loss after $(length(losses)) iterations: $(losses[end])\")\n",
    "\n",
    "# Plot the losses\n",
    "pl_losses = plot(1:200, losses[1:200], yaxis = :log10, xaxis = :log10, xlabel = \"Iterations\", ylabel = \"Loss\", label = \"ADAM\", color = :blue)\n",
    "plot!(201:length(losses), losses[201:end], yaxis = :log10, xaxis = :log10, xlabel = \"Iterations\", ylabel = \"Loss\", label = \"BFGS\", color = :red)\n",
    "savefig(pl_losses, joinpath(pwd(), \"plots\", \"$(svname)_losses.pdf\"))\n",
    "# Rename the best candidate\n",
    "p_trained = res2.minimizer\n",
    "\n",
    "## Analysis of the trained network\n",
    "# Plot the data and the approximation\n",
    "ts = first(solution.t):mean(diff(solution.t))/2:last(solution.t)\n",
    "X̂ = predict(p_trained, Xₙ[:,1], ts)\n",
    "# Trained on noisy data vs real solution\n",
    "pl_trajectory = plot(ts, transpose(X̂), xlabel = \"t\", ylabel =\"x(t), y(t)\", color = :red, label = [\"UDE Approximation\" nothing])\n",
    "scatter!(solution.t, transpose(Xₙ), color = :black, label = [\"Measurements\" nothing])\n",
    "savefig(pl_trajectory, joinpath(pwd(), \"plots\", \"$(svname)_trajectory_reconstruction.pdf\"))\n",
    "\n",
    "# Ideal unknown interactions of the predictor\n",
    "Ȳ = [-p_[2]*(X̂[1,:].*X̂[2,:])';p_[3]*(X̂[1,:].*X̂[2,:])']\n",
    "# Neural network guess\n",
    "Ŷ = U(X̂,p_trained,st)[1]\n",
    "\n",
    "pl_reconstruction = plot(ts, transpose(Ŷ), xlabel = \"t\", ylabel =\"U(x,y)\", color = :red, label = [\"UDE Approximation\" nothing])\n",
    "plot!(ts, transpose(Ȳ), color = :black, label = [\"True Interaction\" nothing])\n",
    "savefig(pl_reconstruction, joinpath(pwd(), \"plots\", \"$(svname)_missingterm_reconstruction.pdf\"))\n",
    "\n",
    "# Plot the error\n",
    "pl_reconstruction_error = plot(ts, norm.(eachcol(Ȳ-Ŷ)), yaxis = :log, xlabel = \"t\", ylabel = \"L2-Error\", label = nothing, color = :red)\n",
    "pl_missing = plot(pl_reconstruction, pl_reconstruction_error, layout = (2,1))\n",
    "savefig(pl_missing, joinpath(pwd(), \"plots\", \"$(svname)_missingterm_reconstruction_and_error.pdf\"))\n",
    "pl_overall = plot(pl_trajectory, pl_missing)\n",
    "savefig(pl_overall, joinpath(pwd(), \"plots\", \"$(svname)_reconstruction.pdf\"))\n",
    "## Symbolic regression via sparse regression ( SINDy based )\n",
    "\n",
    "# Create a Basis\n",
    "@variables u[1:2]\n",
    "# Generate the basis functions, multivariate polynomials up to deg 5\n",
    "# and sine\n",
    "b = [polynomial_basis(u, 5); sin.(u)]\n",
    "basis = Basis(b,u);\n",
    "\n",
    "# Create the thresholds which should be used in the search process\n",
    "λ = exp10.(-3:0.01:5)\n",
    "# Create an optimizer for the SINDy problem\n",
    "opt = STLSQ(λ)\n",
    "# Define different problems for the recovery\n",
    "ideal_problem = DirectDataDrivenProblem(X̂, Ȳ)\n",
    "nn_problem = DirectDataDrivenProblem(X̂, Ŷ)\n",
    "# Test on ideal derivative data for unknown function ( not available )\n",
    "println(\"Sparse regression\")\n",
    "full_res = solve(full_problem, basis, opt, maxiter = 10000, progress = true)\n",
    "ideal_res = solve(ideal_problem, basis, opt, maxiter = 10000, progress = true)\n",
    "nn_res = solve(nn_problem, basis, opt, maxiter = 10000, progress = true, sampler = DataSampler(Batcher(n = 4, shuffle = true)))\n",
    "# Store the results\n",
    "results = [full_res; ideal_res; nn_res]\n",
    "# Show the results\n",
    "map(println, results)\n",
    "# Show the results\n",
    "map(println ∘ result, results)\n",
    "# Show the identified parameters\n",
    "map(println ∘ parameter_map, results)\n",
    "\n",
    "# Define the recovered, hyrid model\n",
    "function recovered_dynamics!(du,u, p, t)\n",
    "    û = nn_res(u, p) # Network prediction\n",
    "    du[1] = p_[1]*u[1] + û[1]\n",
    "    du[2] = -p_[4]*u[2] + û[2]\n",
    "end\n",
    "\n",
    "\n",
    "estimation_prob = ODEProblem(recovered_dynamics!, u0, tspan, parameters(nn_res))\n",
    "estimate = solve(estimation_prob, Tsit5(), saveat = solution.t)\n",
    "\n",
    "# Plot\n",
    "plot(solution)\n",
    "plot!(estimate)\n",
    "\n",
    "## Simulation\n",
    "\n",
    "# Look at long term prediction\n",
    "t_long = (0.0, 50.0)\n",
    "estimation_prob = ODEProblem(recovered_dynamics!, u0, t_long, parameters(nn_res))\n",
    "estimate_long = solve(estimation_prob, Tsit5(), saveat = 0.1) # Using higher tolerances here results in exit of julia\n",
    "plot(estimate_long)\n",
    "\n",
    "true_prob = ODEProblem(lotka!, u0, t_long, p_)\n",
    "true_solution_long = solve(true_prob, Tsit5(), saveat = estimate_long.t)\n",
    "plot!(true_solution_long)\n",
    "\n",
    "## Save the results\n",
    "save(joinpath(pwd(), \"results\" ,\"$(svname)recovery_$(noise_magnitude).jld2\"),\n",
    "    \"solution\", solution, \"X\", Xₙ, \"t\" , ts, \"neural_network\" , U, \"initial_parameters\", p, \"trained_parameters\" , p_trained, # Training\n",
    "    \"losses\", losses, \"result\", nn_res, \"recovered_parameters\", parameters(nn_res), # Recovery\n",
    "    \"long_solution\", true_solution_long, \"long_estimate\", estimate_long) # Estimation\n",
    "\n",
    "\n",
    "## Post Processing and Plots\n",
    "\n",
    "c1 = 3 # RGBA(174/255,192/255,201/255,1) # Maroon\n",
    "c2 = :orange # RGBA(132/255,159/255,173/255,1) # Red\n",
    "c3 = :blue # RGBA(255/255,90/255,0,1) # Orange\n",
    "c4 = :purple # RGBA(153/255,50/255,204/255,1) # Purple\n",
    "\n",
    "p1 = plot(t,abs.(Array(solution) .- estimate)' .+ eps(Float32),\n",
    "          lw = 3, yaxis = :log, title = \"Timeseries of UODE Error\",\n",
    "          color = [3 :orange], xlabel = \"t\",\n",
    "          label = [\"x(t)\" \"y(t)\"],\n",
    "          titlefont = \"Helvetica\", legendfont = \"Helvetica\",\n",
    "          legend = :topright)\n",
    "\n",
    "# Plot L₂\n",
    "p2 = plot3d(X̂[1,:], X̂[2,:], Ŷ[2,:], lw = 3,\n",
    "     title = \"Neural Network Fit of U2(t)\", color = c1,\n",
    "     label = \"Neural Network\", xaxis = \"x\", yaxis=\"y\",\n",
    "     titlefont = \"Helvetica\", legendfont = \"Helvetica\",\n",
    "     legend = :bottomright)\n",
    "plot!(X̂[1,:], X̂[2,:], Ȳ[2,:], lw = 3, label = \"True Missing Term\", color=c2)\n",
    "\n",
    "p3 = scatter(solution, color = [c1 c2], label = [\"x data\" \"y data\"],\n",
    "             title = \"Extrapolated Fit From Short Training Data\",\n",
    "             titlefont = \"Helvetica\", legendfont = \"Helvetica\",\n",
    "             markersize = 5)\n",
    "\n",
    "plot!(p3,true_solution_long, color = [c1 c2], linestyle = :dot, lw=5, label = [\"True x(t)\" \"True y(t)\"])\n",
    "plot!(p3,estimate_long, color = [c3 c4], lw=1, label = [\"Estimated x(t)\" \"Estimated y(t)\"])\n",
    "plot!(p3,[2.99,3.01],[0.0,10.0],lw=1,color=:black, label = nothing)\n",
    "annotate!([(1.5,13,text(\"Training \\nData\", 10, :center, :top, :black, \"Helvetica\"))])\n",
    "l = @layout [grid(1,2)\n",
    "             grid(1,1)]\n",
    "plot(p1,p2,p3,layout = l)\n",
    "savefig(joinpath(pwd(),\"plots\",\"$(svname)full_plot.pdf\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/stevengogogo/Julia-for-SciML/doc-intro-slide?labpath=hands-on%2Flott.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
