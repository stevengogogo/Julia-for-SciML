[
  {
    "objectID": "intro-sciml.html",
    "href": "intro-sciml.html",
    "title": "Part II: SciML Packages",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Session Info",
    "section": "",
    "text": "Place: Blocker 220, Texas A&M University (Map)\nTime: 10:00 am - 12:00 pm. October 25, 2022\n(Latest information is on TAMIDS page)"
  },
  {
    "objectID": "index.html#presenter",
    "href": "index.html#presenter",
    "title": "Session Info",
    "section": "Presenter",
    "text": "Presenter\n\nSteven Shao-Ting Chiu — Ph.D. Student, Department of Electrical Engineering, Texas A&M University\nAdvisor: Dr. Ulisses Braga-Neto"
  },
  {
    "objectID": "index.html#background-and-objectives",
    "href": "index.html#background-and-objectives",
    "title": "Session Info",
    "section": "Background and Objectives",
    "text": "Background and Objectives\nJulia (https://julialang.org/) is a generic programming language designed for high-performance computing. It solves the “two language problem” that typically occurs in scientific computing. Julia is dynamically typed like scripting language such as Python and can be compiled into native machine code. Besides, composablility via multiple dispatches makes Julia works well on the integration across packages. SciML (https://sciml.ai/) is an open-source software for scientific machine learning based on the Julia language that combines machine learning and scientific computing by integrating numerous standalone packages. Notebly, Julia is an open-source project under an MIT license.\nThis worhshop aims to introduce the potential of the Scientific Machine Learning field with Julia programming language. First, we will give an introductory overview of the Julia programming language and explore the Julia SciML ecosystem as an example of its application.\nBoth sessions will include presentations and hands-on sessions. Prior knowledge of Python is recommended, and participants are encouraged to bring their own laptops."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Session Info",
    "section": "Schedule",
    "text": "Schedule\n\nSchedule (Total: 2hrs)\n\n\nTime\nContent\n\n\n\n\n25 min\nIntroduction to the Julia Programming Language\n\n\n25 min\nThe Julia SciML Ecosystem\n\n\n10 min\nBreak\n\n\n30 min\nHands-on session with nerual differential equation\n\n\n30 min\nHands-on session with SciML application"
  },
  {
    "objectID": "index.html#access-to-this-webpage",
    "href": "index.html#access-to-this-webpage",
    "title": "Session Info",
    "section": "Access to this webpage",
    "text": "Access to this webpage\n\n\n\nFigure 1: https://stevengogogo.github.io/Julia-for-SciML\n\n\n\nHands-on session\n\n\n\n\n\n\nDownload hands-on tutorials (Repo, zip)\nView on"
  },
  {
    "objectID": "slides/SLIDE_julia-intro.html#session",
    "href": "slides/SLIDE_julia-intro.html#session",
    "title": "Introduction to Julia Language",
    "section": "Session",
    "text": "Session\n\n\n\n\n\nTime\nContent\n\n\n\n\n25 min\nIntroduction to the Julia Programming Language\n\n\n25 min\nThe Julia SciML Ecosystem\n\n\n10 min\nBreak\n\n\n30 min\nHands-on session with nerual differential equation\n\n\n30 min\nHands-on session with SciML application\n\n\n\n\n\n\n\nFigure 1: Tutorial Website1\n\n\n\n\nhttps://stevengogogo.github.io/Julia-for-SciML"
  },
  {
    "objectID": "slides/SLIDE_julia-intro.html#what-is-julia",
    "href": "slides/SLIDE_julia-intro.html#what-is-julia",
    "title": "Introduction to Julia Language",
    "section": "What is Julia?",
    "text": "What is Julia?\n\n\n\nJulia is a generic high-performance programming language1\n\nJust-In-Time (JIT) compilation\nMultiple dispatch creates type-stability\n\nVia multiple dispatch, different programming patterns can be adapted to the application.\nJulia supports high-level syntax that is approachable."
  },
  {
    "objectID": "slides/SLIDE_julia-intro.html#a-quick-look-at-multiple-dispatch",
    "href": "slides/SLIDE_julia-intro.html#a-quick-look-at-multiple-dispatch",
    "title": "Introduction to Julia Language",
    "section": "A quick look at multiple dispatch",
    "text": "A quick look at multiple dispatch\nA Generic function\njulia> function multiply(a,b)\n           return a*b\n       end\nmultiply (generic function with 1 method)"
  },
  {
    "objectID": "slides/SLIDE_julia-intro.html#a-quick-look-at-multiple-dispatch-1",
    "href": "slides/SLIDE_julia-intro.html#a-quick-look-at-multiple-dispatch-1",
    "title": "Introduction to Julia Language",
    "section": "A quick look at multiple dispatch",
    "text": "A quick look at multiple dispatch\n\nType inference in Julia solves the optimized data type2.\n\nInteger Input\njulia> @code_llvm multiply(1, 1)\n;  @ REPL[1]:1 within `multiply`\ndefine i64 @julia_multiply_660(i64 signext %0, i64 signext %1) #0 {\ntop:\n;  @ REPL[1]:2 within `multiply`\n; ┌ @ int.jl:88 within `*`\n   %2 = mul i64 %1, %0\n; └\n  ret i64 %2\n\n@code_llvm shows the processed script in intermediate presentation (IR).\n\n\nThis is my note.\n\nIt can contain Markdown\nlike this list"
  },
  {
    "objectID": "slides/SLIDE_julia-intro.html#a-quick-look-at-multiple-dispatch-2",
    "href": "slides/SLIDE_julia-intro.html#a-quick-look-at-multiple-dispatch-2",
    "title": "Introduction to Julia Language",
    "section": "A quick look at multiple dispatch",
    "text": "A quick look at multiple dispatch\n\njulia> @code_llvm multiply(2.0,2)\n;  @ REPL[1]:1 within `multiply`\ndefine double @julia_multiply_208(double %0, i64 signext %1) #0 {\ntop:\n;  @ REPL[1]:2 within `multiply`\n; ┌ @ promotion.jl:389 within `*`\n; │┌ @ promotion.jl:359 within `promote`\n; ││┌ @ promotion.jl:336 within `_promote`\n; │││┌ @ number.jl:7 within `convert`\n; ││││┌ @ float.jl:146 within `Float64`\n       %2 = sitofp i64 %1 to double\n# Skip\n  ret double %3\n}"
  },
  {
    "objectID": "slides/SLIDE_julia-intro.html#significant-features",
    "href": "slides/SLIDE_julia-intro.html#significant-features",
    "title": "Introduction to Julia Language",
    "section": "Significant Features",
    "text": "Significant Features\n\nJulia Base and standard library are written in Julia itself.\n\n\n\nJulia is always Just-in-Time (JIT) compiled\n\n\n\n\nMultiple dispatch allows many conbinatoins of argument types\n\nApproaching the speed of statically-compiled language like C/Fortran3\n\n\n\n\nWhy Julia is fast?4\n\nJIT\nType inference\nType specialization in functions"
  },
  {
    "objectID": "slides/SLIDE_julia-intro.html#ecosystem",
    "href": "slides/SLIDE_julia-intro.html#ecosystem",
    "title": "Introduction to Julia Language",
    "section": "Ecosystem",
    "text": "Ecosystem"
  },
  {
    "objectID": "slides/SLIDE_julia-intro.html#references",
    "href": "slides/SLIDE_julia-intro.html#references",
    "title": "Introduction to Julia Language",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n1. Bezanson, J., Edelman, A., Karpinski, S., and Shah, V.B. (2017). Julia: A fresh approach to numerical computing. SIAM review 59, 65–98.\n\n\n2. Nash, J. (2016). Inference Convergence Algorithm in Julia - Julia Computing.\n\n\n3. Julia Documentation \\(\\cdot\\) The Julia Language.\n\n\n4. Rackauckas, C. (2022). Parallel computing and scientific machine learning (SciML): Methods and applications 10.5281/zenodo.6917234."
  },
  {
    "objectID": "apx-resources.html#basics",
    "href": "apx-resources.html#basics",
    "title": "Resources",
    "section": "Basics",
    "text": "Basics\n\nJulia Documentaion (official doc also includes advanced topics)\nThink Julia\nMATLAB-Python-Julia cheatsheet\nJulia cheatsheet\nThe Julia Express"
  },
  {
    "objectID": "apx-resources.html#courses",
    "href": "apx-resources.html#courses",
    "title": "Resources",
    "section": "Courses",
    "text": "Courses\n\nMIT’s 18.337J/6.338J: Parallel Computing and Scientific Machine Learning course. (videos, book)\nMIT 18.S191/6.S083/22.S092 Introduction to Computational Thinking\nA Deep Introduction to Julia for Data Science and Scientific Computing"
  },
  {
    "objectID": "apx-resources.html#community",
    "href": "apx-resources.html#community",
    "title": "Resources",
    "section": "Community",
    "text": "Community\n\nDiscourse\nForum"
  },
  {
    "objectID": "apx-resources.html#packages",
    "href": "apx-resources.html#packages",
    "title": "Resources",
    "section": "Packages",
    "text": "Packages\n\nSurvey of Julia packages"
  },
  {
    "objectID": "apx-resources.html#news",
    "href": "apx-resources.html#news",
    "title": "Resources",
    "section": "News",
    "text": "News\n\nJulia Computing"
  },
  {
    "objectID": "apx-resources.html#julia-platforms",
    "href": "apx-resources.html#julia-platforms",
    "title": "Resources",
    "section": "Julia Platforms",
    "text": "Julia Platforms"
  },
  {
    "objectID": "index-part1.html",
    "href": "index-part1.html",
    "title": "Part I: Overviews",
    "section": "",
    "text": "References\n\n[1] Bezanson, J. et al. 2017. Julia: A fresh approach to numerical computing. SIAM review. 59, 1 (2017), 65–98."
  },
  {
    "objectID": "apx-julia-platforms.html",
    "href": "apx-julia-platforms.html",
    "title": "Julia Platforms",
    "section": "",
    "text": "Julia in Visual Studio Code\njulia-vim"
  },
  {
    "objectID": "apx-julia-platforms.html#demonstration-and-scientific-writing",
    "href": "apx-julia-platforms.html#demonstration-and-scientific-writing",
    "title": "Julia Platforms",
    "section": "Demonstration and Scientific Writing",
    "text": "Demonstration and Scientific Writing\n\nPluto.jl\nQuarto"
  },
  {
    "objectID": "apx-julia-platforms.html#cloud-service",
    "href": "apx-julia-platforms.html#cloud-service",
    "title": "Julia Platforms",
    "section": "Cloud Service",
    "text": "Cloud Service\n\nJuliaHub\nAny VM machines such as Google Cloud Platform and Azure (setup by your own).\nGithub Actions"
  },
  {
    "objectID": "hands-on/julia_basics.html",
    "href": "hands-on/julia_basics.html",
    "title": "TAMIDS SciML Lab Workshop: Julia Softwares for Scientific Machine Learning",
    "section": "",
    "text": "In this section, we are going to run julia with Jupyter\n\n\n\nusing Pkg\nPkg.activate(\"tutorial\")\nPkg.instantiate()\n\n  Activating project at `~/Documents/GitHub/Julia-for-SciML/hands-on/tutorial`\n\n\n\n] st\n\nStatus `~/Documents/GitHub/Julia-for-SciML/hands-on/tutorial/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.3.1\n  [2445eb08] DataDrivenDiffEq v0.8.5\n  [aae7a2af] DiffEqFlux v1.52.0\n  [41bf760c] DiffEqSensitivity v6.79.0\n  [0c46a032] DifferentialEquations v7.5.0\n  [587475ba] Flux v0.13.6\n⌃ [961ee093] ModelingToolkit v8.27.0\n  [429524aa] Optim v1.7.3\n⌃ [7f7a1694] Optimization v3.9.0\n⌃ [91a5bcdd] Plots v1.35.3\nInfo Packages marked with ⌃ have new versions available and may be upgradable.\n\n\n\n\n\n\nusing BenchmarkTools\n\n┌ Info: Precompiling BenchmarkTools [6e4b80f9-dd63-53aa-95a3-0cdb28fa8baf]\n└ @ Base loading.jl:1664\n\n\n\nusing DifferentialEquations\n\n\n\n\n\n?ODEProblem\n\nsearch: ODEProblem RODEProblem SplitODEProblem DynamicalODEProblem\n\n\n\nDefines an ordinary differential equation (ODE) problem. Documentation Page: https://diffeq.sciml.ai/stable/types/ode_types/\n\n\nTo define an ODE Problem, you simply need to give the function \\(f\\) and the initial condition \\(u_0\\) which define an ODE:\n\\[\nM \\frac{du}{dt} = f(u,p,t)\n\\]\nThere are two different ways of specifying f:\n\nf(du,u,p,t): in-place. Memory-efficient when avoiding allocations. Best option for most cases unless mutation is not allowed.\nf(u,p,t): returning du. Less memory-efficient way, particularly suitable when mutation is not allowed (e.g. with certain automatic differentiation packages such as Zygote).\n\nu₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.\nFor the mass matrix \\(M\\), see the documentation of ODEFunction.\n\n\n\n\n\nODEProblem can be constructed by first building an ODEFunction or by simply passing the ODE right-hand side to the constructor. The constructors are:\n\nODEProblem(f::ODEFunction,u0,tspan,p=NullParameters();kwargs...)\nODEProblem{isinplace,specialize}(f,u0,tspan,p=NullParameters();kwargs...) : Defines the ODE with the specified functions. isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred. specialize optionally controls the specialization level. See the specialization levels section of the SciMLBase documentation for more details. The default is AutoSpecialize.\n\nFor more details on the in-place and specialization controls, see the ODEFunction documentation.\nParameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\nFor specifying Jacobians and mass matrices, see the ODEFunction documentation.\n\n\n\n\nf: The function in the ODE.\nu0: The initial condition.\ntspan: The timespan for the problem.\np: The parameters.\nkwargs: The keyword arguments passed onto the solves.\n\n\n\n\n\nExample problems can be found in DiffEqProblemLibrary.jl.\nTo use a sample problem, such as prob_ode_linear, you can do something like:\n#] add DiffEqProblemLibrary\nusing DiffEqProblemLibrary.ODEProblemLibrary\n# load problems\nODEProblemLibrary.importodeproblems()\nprob = ODEProblemLibrary.prob_ode_linear\nsol = solve(prob)\n\nODEProblem(f::ODEFunction,u0,tspan,p=NullParameters(),callback=CallbackSet())\nDefine an ODE problem from an ODEFunction."
  },
  {
    "objectID": "hands-on/lotka.html",
    "href": "hands-on/lotka.html",
    "title": "TAMIDS SciML Lab Workshop: Julia Softwares for Scientific Machine Learning",
    "section": "",
    "text": "Binder\n\n\nSample code is modified from ChrisRackauckas/universsal_differential_equations. This is the part of the work of\n\nRackauckas, Christopher, et al. “Universal differential equations for scientific machine learning.” arXiv preprint arXiv:2001.04385 (2020).\n\n\n\n\n## Environment and packages\ncd(@__DIR__)\nusing Pkg\nPkg.activate(\"lotka\")\n\n  Activating project at `~/Documents/GitHub/Julia-for-SciML/hands-on/lotka`\n\n\n\n@info \"Instantiate\"\nPkg.instantiate() # This step will take a while for numerous packages\n\n┌ Info: Instantiate\n└ @ Main In[2]:1\n┌ Warning: The active manifest file is an older format with no julia version entry. Dependencies may have been resolved with a different julia version.\n└ @ nothing /Users/stevenchiu/Documents/GitHub/Julia-for-SciML/hands-on/lotka/Manifest.toml:0\n\n\n\n] st\n\nStatus `~/Documents/GitHub/Julia-for-SciML/hands-on/lotka/Project.toml`\n  [c3fe647b] AbstractAlgebra v0.27.5\n  [621f4979] AbstractFFTs v1.2.1\n  [1520ce14] AbstractTrees v0.4.3\n  [7d9f7c33] Accessors v0.1.20\n  [79e6a3ab] Adapt v3.4.0\n  [dce04be8] ArgCheck v2.3.0\n⌅ [ec485272] ArnoldiMethod v0.1.0\n  [4fba245c] ArrayInterface v6.0.23\n  [30b0a656] ArrayInterfaceCore v0.1.22\n  [6ba088a2] ArrayInterfaceGPUArrays v0.2.2\n  [015c0d05] ArrayInterfaceOffsetArrays v0.1.6\n  [b0d46f97] ArrayInterfaceStaticArrays v0.1.4\n  [dd5226c6] ArrayInterfaceStaticArraysCore v0.1.3\n  [a2b0951a] ArrayInterfaceTracker v0.1.1\n  [4c555306] ArrayLayouts v0.8.12\n  [15f4f7f2] AutoHashEquals v0.2.0\n  [13072b0f] AxisAlgorithms v1.0.1\n⌅ [ab4f0b2a] BFloat16s v0.2.0\n  [aae01518] BandedMatrices v0.17.7\n  [198e06fe] BangBang v0.3.37\n  [9718e550] Baselet v0.1.1\n  [e2ed5e7c] Bijections v0.1.4\n  [62783981] BitTwiddlingConvenienceFunctions v0.1.4\n  [8e7c35d0] BlockArrays v0.16.21\n  [ffab5731] BlockBandedMatrices v0.11.9\n  [fa961155] CEnum v0.4.2\n  [2a0fbf3d] CPUSummary v0.1.27\n  [00ebfdb7] CSTParser v3.3.6\n  [052768ef] CUDA v3.12.0\n  [49dc2e85] Calculus v0.5.1\n  [7057c7e9] Cassette v0.3.10\n  [082447d4] ChainRules v1.44.7\n  [d360d2e6] ChainRulesCore v1.15.6\n  [9e997f8a] ChangesOfVariables v0.1.4\n  [fb6a15b2] CloseOpenIntervals v0.1.10\n  [944b1d66] CodecZlib v0.7.0\n  [35d6a980] ColorSchemes v3.19.0\n  [3da002f7] ColorTypes v0.11.4\n  [c3611d14] ColorVectorSpace v0.9.9\n  [5ae59095] Colors v0.12.8\n  [861a8166] Combinatorics v1.0.2\n  [a80b9123] CommonMark v0.8.6\n  [38540f10] CommonSolve v0.2.1\n  [bbf7d656] CommonSubexpressions v0.3.0\n  [34da2185] Compat v4.3.0\n  [b0b7db55] ComponentArrays v0.13.4\n  [b152e2b5] CompositeTypes v0.1.2\n  [a33af91c] CompositionsBase v0.1.1\n  [88cd18e8] ConsoleProgressMonitor v0.1.2\n  [187b0558] ConstructionBase v1.4.1\n  [6add18c4] ContextVariablesX v0.1.3\n  [d38c429a] Contour v0.6.2\n  [adafc99b] CpuId v0.3.1\n  [a8cc5b0e] Crayons v4.1.1\n  [9a962f9c] DataAPI v1.12.0\n  [2445eb08] DataDrivenDiffEq v0.8.5\n  [82cc6244] DataInterpolations v3.10.1\n  [864edb3b] DataStructures v0.18.13\n  [e2d170a0] DataValueInterfaces v1.0.0\n  [244e2a9f] DefineSingletons v0.1.2\n  [b429d917] DensityInterface v0.4.0\n  [2b5f629d] DiffEqBase v6.105.1\n  [459566f4] DiffEqCallbacks v2.24.2\n  [c894b116] DiffEqJump v8.6.3\n  [77a26b50] DiffEqNoiseProcess v5.13.1\n  [9fdde737] DiffEqOperators v4.43.1\n  [41bf760c] DiffEqSensitivity v6.79.0\n  [163ba53b] DiffResults v1.1.0\n  [b552c78f] DiffRules v1.12.0\n  [b4f34e82] Distances v0.10.7\n  [31c24e10] Distributions v0.25.76\n  [ced4e74d] DistributionsAD v0.6.43\n⌅ [ffbed154] DocStringExtensions v0.8.6\n  [5b8099bc] DomainSets v0.5.14\n  [fa6b7ba4] DualNumbers v0.6.8\n  [7c1d4256] DynamicPolynomials v0.4.5\n  [da5c29d0] EllipsisNotation v1.6.0\n  [7da242da] Enzyme v0.10.12\n  [d4d017d3] ExponentialUtilities v1.19.0\n  [e2ba6199] ExprTools v0.1.8\n  [c87230d0] FFMPEG v0.4.1\n  [7a1cc6ca] FFTW v1.5.0\n  [cc61a311] FLoops v0.2.1\n  [b9860ae5] FLoopsBase v0.1.1\n  [7034ab61] FastBroadcast v0.2.1\n  [9aa1b823] FastClosures v0.3.2\n  [29a986be] FastLapackInterface v1.2.7\n  [5789e2e9] FileIO v1.16.0\n  [1a297f60] FillArrays v0.13.5\n  [6a86dc24] FiniteDiff v2.15.0\n  [53c48c17] FixedPointNumbers v0.8.4\n  [587475ba] Flux v0.13.6\n  [9c68100b] FoldsThreads v0.1.1\n  [59287772] Formatting v0.4.2\n  [f6369f11] ForwardDiff v0.10.32\n  [069b7b12] FunctionWrappers v1.1.3\n  [d9f16b24] Functors v0.3.0\n  [0c68f7d7] GPUArrays v8.5.0\n  [46192b85] GPUArraysCore v0.1.2\n  [61eb1bfa] GPUCompiler v0.16.4\n  [28b8d3ca] GR v0.69.5\n  [a75be94c] GalacticOptim v3.4.0\n  [c145ed77] GenericSchur v0.5.3\n  [5c1252a2] GeometryBasics v0.4.4\n  [af5da776] GlobalSensitivity v2.1.2\n  [86223c79] Graphs v1.7.4\n  [42e2da0e] Grisu v1.0.2\n  [0b43b601] Groebner v0.2.10\n  [d5909c97] GroupsCore v0.4.0\n  [cd3eb016] HTTP v1.5.0\n  [3e5b6fbb] HostCPUFeatures v0.1.8\n  [0e44f5e4] Hwloc v2.2.0\n  [34004b35] HypergeometricFunctions v0.3.11\n  [b5f81e59] IOCapture v0.2.2\n  [7869d1d1] IRTools v0.4.7\n  [615f187c] IfElse v0.1.1\n  [d25df0c9] Inflate v0.1.3\n  [83e8ac13] IniFile v0.5.1\n  [22cec73e] InitialValues v0.3.1\n  [18e54dd8] IntegerMathUtils v0.1.0\n  [a98d9a8b] Interpolations v0.14.6\n  [8197267c] IntervalSets v0.7.3\n  [3587e190] InverseFunctions v0.1.8\n  [92d709cd] IrrationalConstants v0.1.1\n  [c8e1da08] IterTools v1.4.0\n  [42fd0dbc] IterativeSolvers v0.9.2\n  [82899510] IteratorInterfaceExtensions v1.0.0\n  [033835bb] JLD2 v0.4.25\n  [692b3bcd] JLLWrappers v1.4.1\n  [682c06a0] JSON v0.21.3\n  [98e50ef6] JuliaFormatter v1.0.13\n  [b14d175d] JuliaVariables v0.2.4\n  [ccbc3e58] JumpProcesses v9.2.0\n  [e5e0dc1b] Juno v0.8.4\n⌅ [ef3ab10e] KLU v0.3.0\n  [5ab0869b] KernelDensity v0.6.5\n  [ba0b0d4f] Krylov v0.8.4\n  [0b1a1467] KrylovKit v0.5.4\n  [929cbde3] LLVM v4.14.0\n  [b964fa9f] LaTeXStrings v1.3.0\n  [2ee39098] LabelledArrays v1.12.3\n  [23fbe1c1] Latexify v0.15.17\n  [a5e1c1ea] LatinHypercubeSampling v1.8.0\n  [73f95e8e] LatticeRules v0.0.1\n  [10f19ff3] LayoutPointers v0.1.11\n  [50d2b5c4] Lazy v0.15.1\n  [5078a376] LazyArrays v0.22.12\n⌅ [d7e5e226] LazyBandedMatrices v0.7.17\n  [0fc2ff8b] LeastSquaresOptim v0.8.3\n  [1d6d02ad] LeftChildRightSiblingTrees v0.2.0\n  [2d8b4e74] LevyArea v1.0.0\n  [093fc24a] LightGraphs v1.3.5\n  [d3d80556] LineSearches v7.2.0\n  [7ed4a6bd] LinearSolve v1.27.0\n  [98b081ad] Literate v2.14.0\n  [2ab3a3ac] LogExpFunctions v0.3.18\n  [e6f89c97] LoggingExtras v0.4.9\n  [bdcacae8] LoopVectorization v0.12.136\n  [b2108857] Lux v0.4.29\n  [d8e11817] MLStyle v0.4.14\n  [f1d291b0] MLUtils v0.2.11\n  [1914dd2f] MacroTools v0.5.10\n  [d125e4d3] ManualMemory v0.1.8\n  [a3b82374] MatrixFactorizations v0.9.3\n  [739be429] MbedTLS v1.1.6\n  [eff96d63] Measurements v2.8.0\n  [442fdcdd] Measures v0.3.1\n  [e89f7d12] Media v0.5.0\n  [c03570c3] Memoize v0.4.4\n  [e9d8d322] Metatheory v1.3.5\n  [128add7d] MicroCollections v0.1.3\n  [e1d29d7a] Missings v1.0.2\n  [961ee093] ModelingToolkit v8.29.1\n  [46d2c3a1] MuladdMacro v0.2.2\n  [102ac46a] MultivariatePolynomials v0.4.6\n  [d8a4904e] MutableArithmetics v1.0.5\n  [d41bc354] NLSolversBase v7.8.2\n  [2774e3e8] NLsolve v4.5.1\n  [872c559c] NNlib v0.8.9\n  [a00861dc] NNlibCUDA v0.2.4\n  [77ba4419] NaNMath v1.0.1\n  [71a1bf82] NameResolution v0.1.5\n  [8913a72c] NonlinearSolve v0.3.22\n  [d8793406] ObjectFile v0.3.7\n  [6fe1bfb0] OffsetArrays v1.12.8\n  [429524aa] Optim v1.7.3\n  [3bd65402] Optimisers v0.2.10\n  [7f7a1694] Optimization v3.9.2\n  [36348300] OptimizationOptimJL v0.1.3\n  [42dfb2eb] OptimizationOptimisers v0.1.0\n  [bac558e1] OrderedCollections v1.4.1\n  [1dea7af3] OrdinaryDiffEq v6.29.3\n  [90014a1f] PDMats v0.11.16\n  [d96e819e] Parameters v0.12.3\n  [69de0a69] Parsers v2.4.2\n  [ccf2f8ad] PlotThemes v3.1.0\n  [995b91a9] PlotUtils v1.3.1\n  [91a5bcdd] Plots v1.35.4\n  [e409e4f3] PoissonRandom v0.4.1\n  [f517fe37] Polyester v0.6.16\n  [1d0040c9] PolyesterWeave v0.1.10\n  [85a6dd25] PositiveFactorizations v0.2.4\n  [d236fae5] PreallocationTools v0.4.4\n  [21216c6a] Preferences v1.3.0\n  [8162dcfd] PrettyPrint v0.2.0\n  [27ebfcd6] Primes v0.5.3\n  [33c8b6b6] ProgressLogging v0.1.4\n  [92933f4c] ProgressMeter v1.7.2\n  [1fd47b50] QuadGK v2.5.0\n  [8a4e6c94] QuasiMonteCarlo v0.2.14\n  [74087812] Random123 v1.6.0\n  [fb686558] RandomExtensions v0.4.3\n  [e6cf234a] RandomNumbers v1.5.3\n  [c84ed2f1] Ratios v0.4.3\n  [c1ae055f] RealDot v0.1.0\n  [3cdcf5f2] RecipesBase v1.3.1\n  [01d81517] RecipesPipeline v0.6.7\n  [731186ca] RecursiveArrayTools v2.32.0\n  [f2c3362d] RecursiveFactorization v0.2.12\n  [189a3867] Reexport v1.2.2\n  [42d2dcc6] Referenceables v0.1.2\n  [29dad682] RegularizationTools v0.6.0\n  [05181044] RelocatableFolders v1.0.0\n  [ae029012] Requires v1.3.0\n  [ae5879a3] ResettableStacks v1.1.1\n  [37e2e3b7] ReverseDiff v1.14.4\n  [79098fc4] Rmath v0.7.0\n  [7e49a35a] RuntimeGeneratedFunctions v0.5.3\n  [3cdde19b] SIMDDualNumbers v0.1.1\n  [94e857df] SIMDTypes v0.1.0\n  [476501e8] SLEEFPirates v0.6.36\n  [1bc83da4] SafeTestsets v0.0.1\n  [0bca4576] SciMLBase v1.63.0\n  [6c6a2e73] Scratch v1.1.1\n  [efcf1570] Setfield v1.1.1\n  [605ecd9f] ShowCases v0.1.0\n  [992d4aef] Showoff v1.0.3\n  [777ac1f9] SimpleBufferStream v1.1.0\n  [699a6c99] SimpleTraits v0.9.4\n  [ed01d8cd] Sobol v1.5.0\n  [a2af1166] SortingAlgorithms v1.0.1\n  [47a9eef4] SparseDiffTools v1.27.0\n  [276daf66] SpecialFunctions v2.1.7\n  [171d559e] SplittablesBase v0.1.15\n  [860ef19b] StableRNGs v1.0.0\n  [aedffcd0] Static v0.7.7\n  [90137ffa] StaticArrays v1.5.9\n  [1e83bf80] StaticArraysCore v1.4.0\n  [82ae8749] StatsAPI v1.5.0\n  [2913bbd2] StatsBase v0.33.21\n  [4c63d2b9] StatsFuns v1.0.1\n  [789caeaf] StochasticDiffEq v6.54.0\n  [7792a7ef] StrideArraysCore v0.3.15\n  [09ab397b] StructArrays v0.6.13\n  [53d494c1] StructIO v0.3.0\n  [d1185830] SymbolicUtils v0.19.11\n  [0c5d862f] Symbolics v4.13.0\n  [3783bdb8] TableTraits v1.0.1\n  [bd369af6] Tables v1.10.0\n  [62fd8b95] TensorCore v0.1.1\n⌅ [8ea1fca8] TermInterface v0.2.3\n  [5d786b92] TerminalLoggers v0.1.6\n  [8290d209] ThreadingUtilities v0.5.0\n  [ac1d9e8a] ThreadsX v0.1.11\n  [a759f4b9] TimerOutputs v0.5.21\n  [0796e94c] Tokenize v0.5.24\n  [9f7883ad] Tracker v0.2.22\n  [3bb67fe8] TranscodingStreams v0.9.9\n  [28d57a85] Transducers v0.4.74\n  [592b5752] Trapz v2.0.3\n  [a2a6695c] TreeViews v0.3.0\n  [d5829a12] TriangularSolve v0.1.14\n  [5c2747f8] URIs v1.4.0\n  [3a884ed6] UnPack v1.0.2\n  [d9a01c3f] Underscores v3.0.0\n  [1cfade01] UnicodeFun v0.4.1\n  [1986cc42] Unitful v1.12.0\n  [41fe7b60] Unzip v0.2.0\n  [3d5dd08c] VectorizationBase v0.21.54\n  [19fa3120] VertexSafeGraphs v0.2.0\n  [efce3f68] WoodburyMatrices v0.5.5\n  [a5390f91] ZipFile v0.10.0\n  [e88e6eb3] Zygote v0.6.49\n  [700de1a5] ZygoteRules v0.2.2\n  [6e34b625] Bzip2_jll v1.0.8+0\n  [83423d85] Cairo_jll v1.16.1+1\n  [5ae413db] EarCut_jll v2.2.4+0\n  [7cc45869] Enzyme_jll v0.0.43+0\n  [2e619515] Expat_jll v2.4.8+0\n  [b22a6f82] FFMPEG_jll v4.4.2+2\n  [f5851436] FFTW_jll v3.3.10+0\n  [a3f928ae] Fontconfig_jll v2.13.93+0\n  [d7e528f0] FreeType2_jll v2.10.4+0\n  [559328eb] FriBidi_jll v1.0.10+0\n  [0656b61e] GLFW_jll v3.3.8+0\n  [d2c73de3] GR_jll v0.69.1+0\n  [78b55507] Gettext_jll v0.21.0+0\n  [7746bdde] Glib_jll v2.74.0+1\n  [3b182d85] Graphite2_jll v1.3.14+0\n  [2e76f6c2] HarfBuzz_jll v2.8.1+1\n  [e33a78d0] Hwloc_jll v2.8.0+1\n  [1d5cc7b8] IntelOpenMP_jll v2018.0.3+2\n  [aacddb02] JpegTurbo_jll v2.1.2+0\n  [c1c5ebd0] LAME_jll v3.100.1+0\n  [88015f11] LERC_jll v3.0.0+1\n  [dad2f222] LLVMExtra_jll v0.0.16+0\n  [dd4b983a] LZO_jll v2.10.1+0\n  [dd192d2f] LibVPX_jll v1.10.0+0\n  [e9f186c6] Libffi_jll v3.2.2+1\n  [d4300ac3] Libgcrypt_jll v1.8.7+0\n  [7e76a0d4] Libglvnd_jll v1.3.0+3\n  [7add5ba3] Libgpg_error_jll v1.42.0+0\n  [94ce4f54] Libiconv_jll v1.16.1+1\n  [4b2f31a3] Libmount_jll v2.35.0+0\n  [89763e89] Libtiff_jll v4.4.0+0\n  [38a345b3] Libuuid_jll v2.36.0+0\n  [856f044c] MKL_jll v2022.2.0+0\n  [e7412a2a] Ogg_jll v1.3.5+1\n  [458c3c95] OpenSSL_jll v1.1.17+0\n  [efe28fd5] OpenSpecFun_jll v0.5.5+0\n  [91d4177d] Opus_jll v1.3.2+0\n  [2f80f16e] PCRE_jll v8.44.0+0\n  [30392449] Pixman_jll v0.40.1+0\n  [ea2cea3b] Qt5Base_jll v5.15.3+1\n  [f50d1b31] Rmath_jll v0.3.0+0\n  [a2964d1f] Wayland_jll v1.19.0+0\n  [2381bf8a] Wayland_protocols_jll v1.25.0+0\n  [02c8fc9c] XML2_jll v2.9.14+0\n  [aed1982a] XSLT_jll v1.1.34+0\n  [4f6342f7] Xorg_libX11_jll v1.6.9+4\n  [0c0b7dd1] Xorg_libXau_jll v1.0.9+4\n  [935fb764] Xorg_libXcursor_jll v1.2.0+4\n  [a3789734] Xorg_libXdmcp_jll v1.1.3+4\n  [1082639a] Xorg_libXext_jll v1.3.4+4\n  [d091e8ba] Xorg_libXfixes_jll v5.0.3+4\n  [a51aa0fd] Xorg_libXi_jll v1.7.10+4\n  [d1454406] Xorg_libXinerama_jll v1.1.4+4\n  [ec84b674] Xorg_libXrandr_jll v1.5.2+4\n  [ea2f1a96] Xorg_libXrender_jll v0.9.10+4\n  [14d82f49] Xorg_libpthread_stubs_jll v0.1.0+3\n  [c7cfdc94] Xorg_libxcb_jll v1.13.0+3\n  [cc61e674] Xorg_libxkbfile_jll v1.1.0+4\n  [12413925] Xorg_xcb_util_image_jll v0.4.0+1\n  [2def613f] Xorg_xcb_util_jll v0.4.0+1\n  [975044d2] Xorg_xcb_util_keysyms_jll v0.4.0+1\n  [0d47668e] Xorg_xcb_util_renderutil_jll v0.3.9+1\n  [c22f9ab0] Xorg_xcb_util_wm_jll v0.4.1+1\n  [35661453] Xorg_xkbcomp_jll v1.4.2+4\n  [33bec58e] Xorg_xkeyboard_config_jll v2.27.0+4\n  [c5fb5394] Xorg_xtrans_jll v1.4.0+3\n  [3161d3a3] Zstd_jll v1.5.2+0\n  [a4ae2306] libaom_jll v3.4.0+0\n  [0ac62f75] libass_jll v0.15.1+0\n  [f638f0a6] libfdk_aac_jll v2.0.2+0\n  [b53b4c65] libpng_jll v1.6.38+0\n  [f27f6e37] libvorbis_jll v1.3.7+1\n  [1270edf5] x264_jll v2021.5.5+0\n  [dfaa095f] x265_jll v3.5.0+0\n  [d8fb68d0] xkbcommon_jll v1.4.1+0\n  [0dad84c5] ArgTools v1.1.1\n  [56f22d72] Artifacts\n  [2a0f44e3] Base64\n  [ade2ca70] Dates\n  [8bb1440f] DelimitedFiles\n  [8ba89e20] Distributed\n  [f43a241f] Downloads v1.6.0\n  [7b1f6079] FileWatching\n  [9fa8497b] Future\n  [b77e0a4c] InteractiveUtils\n  [4af54fe1] LazyArtifacts\n  [b27032c2] LibCURL v0.6.3\n  [76f85450] LibGit2\n  [8f399da3] Libdl\n  [37e2e46d] LinearAlgebra\n  [56ddb016] Logging\n  [d6f4376e] Markdown\n  [a63ad114] Mmap\n  [ca575930] NetworkOptions v1.2.0\n  [44cfe95a] Pkg v1.8.0\n  [de0858da] Printf\n  [9abbd945] Profile\n  [3fa0cd96] REPL\n  [9a3f8284] Random\n  [ea8e919c] SHA v0.7.0\n  [9e88b42a] Serialization\n  [1a1011a3] SharedArrays\n  [6462fe0b] Sockets\n  [2f01184e] SparseArrays\n  [10745b16] Statistics\n  [4607b0f0] SuiteSparse\n  [fa267f1f] TOML v1.0.0\n  [a4e569a6] Tar v1.10.1\n  [8dfed614] Test\n  [cf7118a7] UUIDs\n  [4ec0a83e] Unicode\n  [e66e0078] CompilerSupportLibraries_jll v0.5.2+0\n  [deac9b47] LibCURL_jll v7.84.0+0\n  [29816b5a] LibSSH2_jll v1.10.2+0\n  [c8ffd9c3] MbedTLS_jll v2.28.0+0\n  [14a3606d] MozillaCACerts_jll v2022.2.1\n  [4536629a] OpenBLAS_jll v0.3.20+0\n  [05823500] OpenLibm_jll v0.8.1+0\n  [bea87d4a] SuiteSparse_jll v5.10.1+0\n  [83775a58] Zlib_jll v1.2.12+3\n  [8e850b90] libblastrampoline_jll v5.1.1+0\n  [8e850ede] nghttp2_jll v1.48.0+0\n  [3f19e933] p7zip_jll v17.4.0+0\nInfo Packages marked with ⌅ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`\n\n\n\n\n\n\nWhen importing multiple packages, this requires precompilation that takes some time.\n\n\n@info \"Precompile\"\nusing OrdinaryDiffEq\nusing ModelingToolkit\nusing DataDrivenDiffEq\nusing LinearAlgebra, ComponentArrays\nusing Optimization, OptimizationOptimisers, OptimizationOptimJL #OptimizationFlux for ADAM and OptimizationOptimJL for BFGS\nusing DiffEqSensitivity\nusing Lux\nusing Plots\ngr()\nusing Statistics\n\n# Set a random seed for reproduceable behaviour\nusing Random\nrng = Random.default_rng()\nRandom.seed!(1234);\n@info \"Complete Precompilation\"\n\n┌ Info: Precompile\n└ @ Main In[4]:1\n┌ Info: Precompiling ModelingToolkit [961ee093-0014-501f-94e3-6117800e7a78]\n└ @ Base loading.jl:1664\n┌ Info: Precompiling DataDrivenDiffEq [2445eb08-9709-466a-b3fc-47e12bd697a2]\n└ @ Base loading.jl:1664\n┌ Warning: The variable syntax (u[1:n])(t) is deprecated. Use (u(t))[1:n] instead.\n│                   The former creates an array of functions, while the latter creates an array valued function.\n│                   The deprecated syntax will cause an error in the next major release of Symbolics.\n│                   This change will facilitate better implementation of various features of Symbolics.\n└ @ Symbolics ~/.julia/packages/Symbolics/FGTCH/src/variable.jl:129\n┌ Warning: Type annotations on keyword arguments not currently supported in recipes. Type information has been discarded\n└ @ RecipesBase ~/.julia/packages/RecipesBase/6AijY/src/RecipesBase.jl:117\n┌ Info: Precompiling Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]\n└ @ Base loading.jl:1664\n┌ Info: Complete Precompilation\n└ @ Main In[4]:17\n\n\n\n\n\nFor simplicity, we use Lotka-Volterra system as an example\n\\[\\begin{align}\n    \\dot{x} &= \\alpha x - \\beta xy\\\\\n    \\dot{y} &= \\gamma xy- \\delta y\n\\end{align}\\]\nwhere \\(\\alpha, \\beta, \\gamma\\), and \\(\\delta\\) are positive real parameters\n\n## Data generation\nfunction lotka!(du, u, p, t)\n    α, β, γ, δ = p\n    du[1] = α*u[1] - β*u[2]*u[1]\n    du[2] = γ*u[1]*u[2]  - δ*u[2]\nend\n\nlotka! (generic function with 1 method)\n\n\n\n# Define the experimental parameter\ntspan = (0.0,3.0)\nu0 = [0.44249296,4.6280594]\np_ = [1.3, 0.9, 0.8, 1.8]\n\n4-element Vector{Float64}:\n 1.3\n 0.9\n 0.8\n 1.8\n\n\n\n\n\nVern7 is used for non-stiff problems. Numerous solvers can be found on DifferentialEquations.jl’s doc\n\n\n# Solve\nprob = ODEProblem(lotka!, u0,tspan, p_)\nsolution = solve(prob, Vern7(), abstol=1e-12, reltol=1e-12, saveat = 0.1)\n\nretcode: Success\nInterpolation: 1st order linear\nt: 31-element Vector{Float64}:\n 0.0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1.0\n 1.1\n 1.2\n ⋮\n 1.9\n 2.0\n 2.1\n 2.2\n 2.3\n 2.4\n 2.5\n 2.6\n 2.7\n 2.8\n 2.9\n 3.0\nu: 31-element Vector{Vector{Float64}}:\n [0.44249296, 4.6280594]\n [0.34212452862086234, 3.98764547181634]\n [0.2793966078254349, 3.4139529441083147]\n [0.2394952228707143, 2.9110318130603883]\n [0.21413620714095402, 2.4758280205419836]\n [0.19854852659179129, 2.1022922430734137]\n [0.18991187927524103, 1.7834096349202704]\n [0.18652973211225643, 1.5121821427640152]\n [0.18737918127509637, 1.2820806846455604]\n [0.1918587411736629, 1.087227597605956]\n [0.1996432344128222, 0.9224424008592909]\n [0.2105985019620811, 0.7832199752377471]\n [0.22473063540355143, 0.6656774980182895]\n ⋮\n [0.4333056937367298, 0.22471175932636067]\n [0.48425346211989406, 0.1947029152564331]\n [0.5425361548950363, 0.16943926722620506]\n [0.6091040110729008, 0.14819092695665834]\n [0.6850407509453579, 0.13034710141497852]\n [0.7715795653361799, 0.11539841080610512]\n [0.8701212001899306, 0.10292221843899205]\n [0.9822541897624152, 0.09257065810821445]\n [1.1097772412678872, 0.08406114122763123]\n [1.2547236687788759, 0.07716924328704818]\n [1.419387582491876, 0.07172402271816045]\n [1.606351205697802, 0.06760604257226555]\n\n\n\n\n\n\n\n# Ideal data\nX = Array(solution)\nt = solution.t\nDX = Array(solution(solution.t, Val{1}))\n\nfull_problem = DataDrivenProblem(X, t = t, DX = DX)\n\n# Add noise in terms of the mean\nx̄ = mean(X, dims = 2)\nnoise_magnitude = 5e-3\nXₙ = X .+ (noise_magnitude*x̄) .* randn(eltype(X), size(X))\n\n2×31 Matrix{Float64}:\n 0.444955  0.344412  0.277873  0.246362  …  1.2542     1.41631    1.60945\n 4.6231    3.98748   3.40663   2.91876      0.0810739  0.0772539  0.0672219\n\n\n\n\n\n\nplot(solution, alpha = 0.75, color = :black, label = [\"True Data\" nothing])\nscatter!(t, transpose(Xₙ), color = :red, label = [\"Noisy Data\" nothing])\n\n\n\n\n\n\n\nSuppose we only know part of the Lotka-Voltera model, and use CNN to surrogate the unknown part\n\\[\\begin{align}\n    \\dot{x} &= \\theta_1 x + U_1(\\theta_3, x, y)\\\\\n    \\dot{y} &= -\\theta_2 y + U_2(\\theta_3..., x, y)\n\\end{align}\\]\n\n## Define the network\n# Gaussian RBF as activation\nrbf(x) = exp.(-(x.^2))\n\n# Multilayer FeedForward\nU = Lux.Chain(\n    Lux.Dense(2,5,rbf), Lux.Dense(5,5, rbf), Lux.Dense(5,5, rbf), Lux.Dense(5,2)\n)\n# Get the initial parameters and state variables of the model\np, st = Lux.setup(rng, U)\n\n# Define the hybrid model\nfunction ude_dynamics!(du,u, p, t, p_true)\n    û = U(u, p, st)[1] # Network prediction\n    du[1] = p_true[1]*u[1] + û[1]\n    du[2] = -p_true[4]*u[2] + û[2]\nend\n\nude_dynamics! (generic function with 1 method)\n\n\n\n# Closure with the known parameter\nnn_dynamics!(du,u,p,t) = ude_dynamics!(du,u,p,t,p_)\n# Define the problem (Fix: https://discourse.julialang.org/t/issue-with-ude-repository-lv-scenario-1/88618/5)\nprob_nn = ODEProblem{true, SciMLBase.FullSpecialize}(nn_dynamics!,Xₙ[:, 1], tspan, p)\n#prob_nn = ODEProblem(nn_dynamics!,Xₙ[:, 1], tspan, p)\n\n\nODEProblem with uType Vector{Float64} and tType Float64. In-place: true\ntimespan: (0.0, 3.0)\nu0: 2-element Vector{Float64}:\n 0.44495468189157616\n 4.623098367786485\n\n\n\n\n\n\n\n## Function to train the network\n# Define a predictor\nfunction predict(θ, X = Xₙ[:,1], T = t)\n    _prob = remake(prob_nn, u0 = X, tspan = (T[1], T[end]), p = θ)\n    Array(solve(_prob, Vern7(), saveat = T,\n                abstol=1e-6, reltol=1e-6,\n                sensealg = ForwardDiffSensitivity()\n                ))\nend\n\n# Simple L2 loss\nfunction loss(θ)\n    X̂ = predict(θ)\n    sum(abs2, Xₙ .- X̂)\nend\n\n# Container to track the losses\nlosses = Float64[]\n\ncallback = function (p, l)\n  push!(losses, l)\n  if length(losses)%50==0\n      println(\"Current loss after $(length(losses)) iterations: $(losses[end])\")\n  end\n  return false\nend\n\n#1 (generic function with 1 method)\n\n\n\n\n\nThe training is splitted to two steps: 1. ADAM: for better convergence 2. BFGS: get better position\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, ComponentVector{Float64}(p))\nres1 = Optimization.solve(optprob, ADAM(0.1), callback=callback, maxiters = 200)\n\n@info \"Training loss after $(length(losses)) iterations: $(losses[end])\"\n# Train with BFGS\n@time optprob2 = Optimization.OptimizationProblem(optf, res1.minimizer)\n@time res2 = Optimization.solve(optprob2, Optim.BFGS(initial_stepnorm=0.01), callback=callback, maxiters = 3000)\n@info \"Final training loss after $(length(losses)) iterations: $(losses[end])\"\n\nCurrent loss after 50 iterations: 3.2919945635723753\nCurrent loss after 100 iterations: 1.7058558916650455\nCurrent loss after 150 iterations: 1.6697049368588788\nCurrent loss after 200 iterations: 1.6423084245679131\n  0.002839 seconds (946 allocations: 59.323 KiB, 97.18% compilation time)\n\n\n┌ Info: Training loss after 201 iterations: 1.6423084245679131\n└ @ Main In[13]:6\n\n\nCurrent loss after 250 iterations: 0.023661287470332845\nCurrent loss after 300 iterations: 0.013746817618177573\nCurrent loss after 350 iterations: 0.0032781193503113072\nCurrent loss after 400 iterations: 0.0017736994450182483\nCurrent loss after 450 iterations: 0.0016452061747373918\nCurrent loss after 500 iterations: 0.001415755561505311\nCurrent loss after 550 iterations: 0.001226557886040906\nCurrent loss after 600 iterations: 0.001107089612914693\nCurrent loss after 650 iterations: 0.0009982934123468784\nCurrent loss after 700 iterations: 0.0009875749757684356\nCurrent loss after 750 iterations: 0.0009812594981828098\nCurrent loss after 800 iterations: 0.0009796658875397177\nCurrent loss after 850 iterations: 0.0009793537523280391\nCurrent loss after 900 iterations: 0.0009772977493860333\nCurrent loss after 950 iterations: 0.0009743301969273224\nCurrent loss after 1000 iterations: 0.0009727084653298941\nCurrent loss after 1050 iterations: 0.000972080483835656\nCurrent loss after 1100 iterations: 0.0009717534715880698\nCurrent loss after 1150 iterations: 0.0009656859249757323\nCurrent loss after 1200 iterations: 0.000962561381268262\nCurrent loss after 1250 iterations: 0.0009612859817110582\nCurrent loss after 1300 iterations: 0.0009591475053966475\nCurrent loss after 1350 iterations: 0.0009582234874165839\nCurrent loss after 1400 iterations: 0.0009574676566527876\nCurrent loss after 1450 iterations: 0.0009569122026897299\nCurrent loss after 1500 iterations: 0.0009567527113130108\nCurrent loss after 1550 iterations: 0.0009561199657093553\nCurrent loss after 1600 iterations: 0.0009549884254749657\nCurrent loss after 1650 iterations: 0.0009541600089060593\nCurrent loss after 1700 iterations: 0.0009537452602046372\nCurrent loss after 1750 iterations: 0.0009536569492935126\nCurrent loss after 1800 iterations: 0.0009534426125349452\nCurrent loss after 1850 iterations: 0.0009532818826279794\nCurrent loss after 1900 iterations: 0.0009530863107623305\nCurrent loss after 1950 iterations: 0.0009529698205412621\nCurrent loss after 2000 iterations: 0.0009528567762890976\nCurrent loss after 2050 iterations: 0.0009525373206446654\nCurrent loss after 2100 iterations: 0.0009521593217738839\nCurrent loss after 2150 iterations: 0.0009520958641338131\nCurrent loss after 2200 iterations: 0.0009520568498258301\nCurrent loss after 2250 iterations: 0.0009517387568709354\nCurrent loss after 2300 iterations: 0.0009516121690135998\nCurrent loss after 2350 iterations: 0.0009514360660860543\nCurrent loss after 2400 iterations: 0.0009512526383059336\nCurrent loss after 2450 iterations: 0.0009511495748527354\nCurrent loss after 2500 iterations: 0.000951106817478341\nCurrent loss after 2550 iterations: 0.0009510985159145336\nCurrent loss after 2600 iterations: 0.0009509270459168483\nCurrent loss after 2650 iterations: 0.000950863437522074\nCurrent loss after 2700 iterations: 0.0009507791033767345\nCurrent loss after 2750 iterations: 0.000950657136744276\nCurrent loss after 2800 iterations: 0.0009506293508157388\nCurrent loss after 2850 iterations: 0.0009506101911525282\nCurrent loss after 2900 iterations: 0.0009504534699507922\nCurrent loss after 2950 iterations: 0.0009504069207206256\nCurrent loss after 3000 iterations: 0.0009503383212540466\nCurrent loss after 3050 iterations: 0.0009503196958639988\nCurrent loss after 3100 iterations: 0.0009502399212363976\nCurrent loss after 3150 iterations: 0.0009501939332356801\nCurrent loss after 3200 iterations: 0.0009501728013450467\n980.071985 seconds (629.24 M allocations: 116.047 GiB, 1.16% gc time, 0.19% compilation time)\n\n\n┌ Info: Final training loss after 3202 iterations: 0.0009501727548593611\n└ @ Main In[13]:10\n\n\n\n\n\n\n# Plot the losses\npl_losses = plot(1:200, losses[1:200], yaxis = :log10, xaxis = :log10, xlabel = \"Iterations\", ylabel = \"Loss\", label = \"ADAM\", color = :blue)\nplot!(201:length(losses), losses[201:end], yaxis = :log10, xaxis = :log10, xlabel = \"Iterations\", ylabel = \"Loss\", label = \"BFGS\", color = :red)\n\n\n\n\n\n# Rename the best candidate\np_trained = res2.minimizer;\n\n\n## Analysis of the trained network\n# Plot the data and the approximation\nts = first(solution.t):mean(diff(solution.t))/2:last(solution.t)\nX̂ = predict(p_trained, Xₙ[:,1], ts)\n# Trained on noisy data vs real solution\npl_trajectory = plot(ts, transpose(X̂), xlabel = \"t\", ylabel =\"x(t), y(t)\", color = :red, label = [\"UDE Approximation\" nothing])\nscatter!(solution.t, transpose(Xₙ), color = :black, label = [\"Measurements\" nothing])\n\n# Ideal unknown interactions of the predictor\nȲ = [-p_[2]*(X̂[1,:].*X̂[2,:])';p_[3]*(X̂[1,:].*X̂[2,:])']\n# Neural network guess\nŶ = U(X̂,p_trained,st)[1]\n\npl_reconstruction = plot(ts, transpose(Ŷ), xlabel = \"t\", ylabel =\"U(x,y)\", color = :red, label = [\"UDE Approximation\" nothing])\nplot!(ts, transpose(Ȳ), color = :black, label = [\"True Interaction\" nothing])\n\n# Plot the error\npl_reconstruction_error = plot(ts, norm.(eachcol(Ȳ-Ŷ)), yaxis = :log, xlabel = \"t\", ylabel = \"L2-Error\", label = nothing, color = :red)\npl_missing = plot(pl_reconstruction, pl_reconstruction_error, layout = (2,1))\n\npl_overall = plot(pl_trajectory, pl_missing)\n## Symbolic regression via sparse regression ( SINDy based )\n\n\n\n\n\n\n\n\n# Create a Basis\n@variables u[1:2]\n# Generate the basis functions, multivariate polynomials up to deg 5\n# and sine\nb = [polynomial_basis(u, 5); sin.(u)]\nbasis = Basis(b,u);\n\n# Create the thresholds which should be used in the search process\nλ = exp10.(-3:0.01:5)\n# Create an optimizer for the SINDy problem\nopt = STLSQ(λ)\n\n# Define different problems for the recovery\nideal_problem = DirectDataDrivenProblem(X̂, Ȳ)\nnn_problem = DirectDataDrivenProblem(X̂, Ŷ)\n\n# Test on ideal derivative data for unknown function ( not available )\nprintln(\"Sparse regression\")\nfull_res = solve(full_problem, basis, opt, maxiter = 10000, progress = true)\nideal_res = solve(ideal_problem, basis, opt, maxiter = 10000, progress = true)\nnn_res = solve(nn_problem, basis, opt, maxiter = 10000, progress = true, sampler = DataSampler(Batcher(n = 4, shuffle = true)))\n\n\n# Store the results\nresults = [full_res; ideal_res; nn_res]\n# Show the results\nmap(println, results)\n# Show the results\nmap(println ∘ result, results)\n# Show the identified parameters\nmap(println ∘ parameter_map, results)\n\n# Define the recovered, hyrid model\nfunction recovered_dynamics!(du,u, p, t)\n    û = nn_res(u, p) # Network prediction\n    du[1] = p_[1]*u[1] + û[1]\n    du[2] = -p_[4]*u[2] + û[2]\nend\n\n\nestimation_prob = ODEProblem(recovered_dynamics!, u0, tspan, parameters(nn_res))\nestimate = solve(estimation_prob, Tsit5(), saveat = solution.t)\n\nSparse regression\n\n\nSTLSQ   0%|▏                                             |  ETA: 0:04:27\n  Threshold:          0.0010232929922807535\n  Best Objective:     0.0\n  Best Sparsity:      23.0\n  Current Objective:  0.0\nSTLSQ   0%|▏                                             |  ETA: 0:02:41\n  Threshold:          0.0010232929922807535\n  Best Objective:     0.0\n  Best Sparsity:      23.0\n  Current Objective:  0.0\n  Current Sparsity:   23.0\n\n\nLinear Solution with 2 equations and 20 parameters.\nReturncode: solved\nL₂ Norm error : [31.995291148539735, 1.2046710278865183]\nAIC : [147.43325095127574, 45.77240223675897]\nR² : [-1.2429510122420595, 0.990118386736816]\n\nLinear Solution with 2 equations and 2 parameters.\nReturncode: solved\nL₂ Norm error : [8.108165065870263e-32, 1.3731880503480992e-31]\nAIC : [-4362.980934730921, -4330.843170940274]\nR² : [1.0, 1.0]\n\nLinear Solution with 2 equations and 3 parameters.\nReturncode: solved\nL₂ Norm error : [1.6430538154747572, 8.039350195270902]\nAIC : [36.28995216787681, 133.14524376489635]\nR² : [0.7906825418979999, -0.31134160491819185]\n\nModel ##Basis#629 with 2 equations\nStates : u[1] u[2]\nParameters : 20\nIndependent variable: t\nEquations\nDifferential(t)(u[1]) = p₁ + p₁₀*(u[2]^2) + p₃*(u[1]^2) + p₁₇*sin(u[1]) + p₂*u[1] + p₄*(u[1]^3) + p₅*u[2] + p₁₂*(u[1]^2)*(u[2]^2) + p₁₅*(u[1]^2)*(u[2]^3) + p₁₃*(u[1]^3)*(u[2]^2) + p₁₁*(u[2]^2)*u[1] + p₁₄*(u[2]^3)*u[1] + p₇*(u[1]^2)*u[2] + p₈*(u[1]^3)*u[2] + p₉*(u[1]^4)*u[2] + p₁₆*(u[2]^4)*u[1] + p₆*u[1]*u[2]\nDifferential(t)(u[2]) = p₁₉*(u[1]^2)*u[2] + p₂₀*(u[1]^3)*u[2] + p₁₈*u[1]*u[2]\nModel ##Basis#632 with 2 equations\nStates : u[1] u[2]\nParameters : p₁ p₂\nIndependent variable: t\nEquations\nφ₁ = p₁*u[1]*u[2]\nφ₂ = p₂*u[1]*u[2]\nModel ##Basis#635 with 2 equations\nStates : u[1] u[2]\nParameters : p₁ p₂ p₃\nIndependent variable: t\nEquations\nφ₁ = p₁*(u[1]^2)*u[2]\nφ₂ = p₃*sin(u[1]) + p₂*u[1]\nPair{Sym{Real, Base.ImmutableDict{DataType, Any}}, Float64}[p₁ => 88.0, p₂ => 90.1, p₃ => 45.4, p₄ => 27.6, p₅ => 73.5, p₆ => -1107.4, p₇ => -2835.6, p₈ => 27.9, p₉ => 25.07, p₁₀ => 16.9, p₁₁ => -472.5, p₁₂ => 6115.8, p₁₃ => -117.4, p₁₄ => 22.016, p₁₅ => -659.7, p₁₆ => -25.4, p₁₇ => 62.5, p₁₈ => -13.9, p₁₉ => 31.25, p₂₀ => -15.5]\nPair{Sym{Real, Base.ImmutableDict{DataType, Any}}, Float64}[p₁ => -0.9, p₂ => 0.8]\nPair{Sym{Real, Base.ImmutableDict{DataType, Any}}, Float64}[p₁ => -2.012, p₂ => -2.25, p₃ => 3.3]\n\n\nretcode: Success\nInterpolation: 1st order linear\nt: 31-element Vector{Float64}:\n 0.0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1.0\n 1.1\n 1.2\n ⋮\n 1.9\n 2.0\n 2.1\n 2.2\n 2.3\n 2.4\n 2.5\n 2.6\n 2.7\n 2.8\n 2.9\n 3.0\nu: 31-element Vector{Vector{Float64}}:\n [0.44249296, 4.6280594]\n [0.3589998389049996, 3.90043838799414]\n [0.32039236584298314, 3.288366022606705]\n [0.30209737580427426, 2.7749501847648586]\n [0.29519534196248753, 2.345125591932463]\n [0.2957319262240388, 1.9858702305186864]\n [0.30173746136416507, 1.6860822316973967]\n [0.3121621269423411, 1.4363640495034167]\n [0.3264408593862669, 1.228791374981506]\n [0.3442811760621213, 1.05671475218947]\n [0.3655278350988628, 0.9145062682055913]\n [0.3901432875306497, 0.7974760103603659]\n [0.41813107826611684, 0.7016575070929881]\n ⋮\n [0.7074222345154149, 0.3968840102725108]\n [0.7606094385047732, 0.3826360576519005]\n [0.8159667970667692, 0.3714306439202008]\n [0.8731544682099793, 0.3621229881052103]\n [0.931833648528594, 0.35361775010135293]\n [0.9918281148809143, 0.34494926059255426]\n [1.053178574026386, 0.33524417437964893]\n [1.1161906808048632, 0.3236127866772301]\n [1.1815090141176419, 0.3091482705104759]\n [1.250273983171013, 0.2909351024621833]\n [1.3243227902154364, 0.26776728987299037]\n [1.4068247104939124, 0.23796649830740021]\n\n\n\n\n\n\n# Plot\nplot(solution)\nplot!(estimate)\n\n## Simulation\n\n# Look at long term prediction\nt_long = (0.0, 50.0)\nestimation_prob = ODEProblem(recovered_dynamics!, u0, t_long, parameters(nn_res))\nestimate_long = solve(estimation_prob, Tsit5()) # Using higher tolerances here results in exit of julia\nplot(estimate_long)\n\ntrue_prob = ODEProblem(lotka!, u0, t_long, p_)\ntrue_solution_long = solve(true_prob, Tsit5(), saveat = estimate_long.t)\nplot!(true_solution_long)\n\n\n\n## Post Processing and Plots\n\nc1 = 3 # RGBA(174/255,192/255,201/255,1) # Maroon\nc2 = :orange # RGBA(132/255,159/255,173/255,1) # Red\nc3 = :blue # RGBA(255/255,90/255,0,1) # Orange\nc4 = :purple # RGBA(153/255,50/255,204/255,1) # Purple\n\np1 = plot(t,abs.(Array(solution) .- estimate)' .+ eps(Float32),\n          lw = 3, yaxis = :log, title = \"Timeseries of UODE Error\",\n          color = [3 :orange], xlabel = \"t\",\n          label = [\"x(t)\" \"y(t)\"],\n          titlefont = \"Helvetica\", legendfont = \"Helvetica\",\n          legend = :topright)\n\n# Plot L₂\np2 = plot3d(X̂[1,:], X̂[2,:], Ŷ[2,:], lw = 3,\n     title = \"Neural Network Fit of U2(t)\", color = c1,\n     label = \"Neural Network\", xaxis = \"x\", yaxis=\"y\",\n     titlefont = \"Helvetica\", legendfont = \"Helvetica\",\n     legend = :bottomright)\nplot!(X̂[1,:], X̂[2,:], Ȳ[2,:], lw = 3, label = \"True Missing Term\", color=c2)\n\np3 = scatter(solution, color = [c1 c2], label = [\"x data\" \"y data\"],\n             title = \"Extrapolated Fit From Short Training Data\",\n             titlefont = \"Helvetica\", legendfont = \"Helvetica\",\n             markersize = 5)\n\nplot!(p3,true_solution_long, color = [c1 c2], linestyle = :dot, lw=5, label = [\"True x(t)\" \"True y(t)\"])\nplot!(p3,estimate_long, color = [c3 c4], lw=1, label = [\"Estimated x(t)\" \"Estimated y(t)\"])\nplot!(p3,[2.99,3.01],[0.0,10.0],lw=1,color=:black, label = nothing)\nannotate!([(1.5,13,text(\"Training \\nData\", 10, :center, :top, :black, \"Helvetica\"))])\nl = @layout [grid(1,2)\n             grid(1,1)]\nplot(p1,p2,p3,layout = l)\n\n┌ Warning: dt(7.105427357601002e-15) <= dtmin(7.105427357601002e-15) at t=3.6948134503799572. Aborting. There is either an error in your model specification or the true solution is unstable.\n└ @ SciMLBase /Users/stevenchiu/.julia/packages/SciMLBase/kTnku/src/integrator_interface.jl:516"
  },
  {
    "objectID": "hands-on/stack/infectious_model.html",
    "href": "hands-on/stack/infectious_model.html",
    "title": "Example: Infectious Model",
    "section": "",
    "text": "cd(@__DIR__)\nusing Pkg\nPkg.activate(\"tutorial\")\nPkg.instantiate()\n\n  Activating project at `~/Documents/GitHub/Julia-for-SciML/hands-on/tutorial`\n\n\n\nusing DifferentialEquations\nusing ModelingToolkit\nusing DataDrivenDiffEq\nusing LinearAlgebra, DiffEqSensitivity, Optim\nusing DiffEqFlux, Flux\nusing Plots\nusing Optimization\nimport DiffEqSensitivity as ds\nimport DiffEqFlux as df\nimport Optimization as op\ngr();\n\n\nfunction corona!(du,u,p,t)\n    S,E,I,R,N,D,C = u\n    F, β0,α,κ,μ,σ,γ,d,λ = p\n    dS = -β0*S*F/N - β(t,β0,D,N,κ,α)*S*I/N -μ*S # susceptible\n    dE = β0*S*F/N + β(t,β0,D,N,κ,α)*S*I/N -(σ+μ)*E # exposed\n    dI = σ*E - (γ+μ)*I # infected\n    dR = γ*I - μ*R # removed (recovered + dead)\n    dN = -μ*N # total population\n    dD = d*γ*I - λ*D # severe, critical cases, and deaths\n    dC = σ*E # +cumulative cases\n\n    du[1] = dS; du[2] = dE; du[3] = dI; du[4] = dR\n    du[5] = dN; du[6] = dD; du[7] = dC\nend\nβ(t,β0,D,N,κ,α) = β0*(1-α)*(1-D/N)^κ\nS0 = 14e6\nu0 = [0.9*S0, 0.0, 0.0, 0.0, S0, 0.0, 0.0]\np_ = [10.0, 0.5944, 0.4239, 1117.3, 0.02, 1/3, 1/5,0.2, 1/11.2]\nR0 = p_[2]/p_[7]*p_[6]/(p_[6]+p_[5])\ntspan = (0.0, 21.0)\nprob = ODEProblem(corona!, u0, tspan, p_)\nsolution = solve(prob, Vern7(), abstol=1e-12, reltol=1e-12, saveat = 1)\n\ntspan2 = (0.0,60.0)\nprob = ODEProblem(corona!, u0, tspan2, p_)\nsolution_extrapolate = solve(prob, Vern7(), abstol=1e-12, reltol=1e-12, saveat = 1)\n\n# Ideal data\ntsdata = Array(solution)\n# Add noise to the data\nnoisy_data = tsdata + Float32(1e-5)*randn(eltype(tsdata), size(tsdata))\n\n7×22 Matrix{Float64}:\n  1.26e7      1.23505e7  1.21059e7   1.18662e7  …    8.44577e6    8.2785e6\n  9.03374e-6  4.5798     8.14608    11.2091         70.9046      75.7781\n  1.76877e-5  0.744565   2.53841     4.95787        79.5931      85.8416\n  1.4555e-5   0.0516033  0.363191    1.09095       115.541      129.63\n  1.4e7       1.37228e7  1.34511e7   1.31847e7       9.38448e6    9.19866e6\n -1.19366e-6  0.0101378  0.0700881   0.206669   …   16.4648      18.2247\n -1.69442e-6  0.801628   2.94212     6.17725       221.823      246.265\n\n\n\nplot(abs.(tsdata-noisy_data)')\n\n\n\n\n\n### Neural ODE\n\nann_node = FastChain(FastDense(7, 64, tanh),FastDense(64, 64, tanh), FastDense(64, 64, tanh), FastDense(64, 7))\np = Float64.(initial_params(ann_node))\n\nfunction dudt_node(u,p,t)\n    S,E,I,R,N,D,C = u\n    F,β0,α,κ,μ,σ,γ,d,λ = p_\n    dS,dE,dI,dR,dD = ann_node([S/N,E,I,R,N,D/N,C],p)\n\n    dN = -μ*N # total population\n    dC = σ*E # +cumulative cases\n\n    [dS,dE,dI,dR,dN,dD,dC]\nend\nprob_node = ODEProblem(dudt_node, u0, tspan, p)\ns = concrete_solve(prob_node, Tsit5(), u0, p, saveat = solution.t)\n\nfunction predict(θ)\n    Array(concrete_solve(prob_node, Vern7(), u0, θ, saveat = 1,\n                         abstol=1e-6, reltol=1e-6,\n                         sensealg = SciMLSensitivity.InterpolatingAdjoint(autojacvec=SciMLSensitivity.ReverseDiffVJP())))\nend\n\n# No regularisation right now\nfunction loss(θ)\n    pred = predict(θ)\n    sum(abs2, (noisy_data[2:4,:] .- pred[2:4,:])), pred # + 1e-5*sum(sum.(abs, params(ann)))\nend\n\nloss(p)\n\nconst losses = []\ncallback(θ,l,pred) = begin\n    push!(losses, l)\n    if length(losses)%50==0\n        println(losses[end])\n    end\n    false\nend\n\n┌ Warning: FastChain is being deprecated in favor of Lux.jl. Lux.jl uses functions with explicit parameters f(u,p) like FastChain, but is fully featured and documented machine learning library. See the Lux.jl documentation for more details.\n└ @ DiffEqFlux /Users/stevenchiu/.julia/packages/DiffEqFlux/Em1Aj/src/fast_layers.jl:9\nWARNING: redefinition of constant losses. This may fail, cause incorrect answers, or produce other errors.\n\n\ncallback (generic function with 1 method)\n\n\n\nres1_node = DiffEqFlux.sciml_train(loss, p, ADAM(0.01), cb=callback, maxiters = 500);\n\n┌ Warning: sciml_train is being deprecated in favor of direct usage of Optimization.jl. Please consult the Optimization.jl documentation for more details. Optimization.jl's PolyOpt solver is the polyalgorithm of sciml_train\n└ @ DiffEqFlux /Users/stevenchiu/.julia/packages/DiffEqFlux/Em1Aj/src/train.jl:6\n\n\nLoadError: Need an adjoint for constructor SciMLSensitivity.InterpolatingAdjoint{0, true, Val{:central}, SciMLSensitivity.ReverseDiffVJP{false}}. Gradient is of type ChainRulesCore.ZeroTangent\n\n\n\nres2_node = DiffEqFlux.sciml_train(loss, res1_node.minimizer, BFGS(initial_stepnorm=0.01), cb=callback, maxiters = 10000);\n\n┌ Warning: sciml_train is being deprecated in favor of direct usage of Optimization.jl. Please consult the Optimization.jl documentation for more details. Optimization.jl's PolyOpt solver is the polyalgorithm of sciml_train\n└ @ DiffEqFlux /Users/stevenchiu/.julia/packages/DiffEqFlux/Em1Aj/src/train.jl:6\n\n\nLoadError: Need an adjoint for constructor SciMLSensitivity.InterpolatingAdjoint{0, true, Val{:central}, SciMLSensitivity.ReverseDiffVJP{false}}. Gradient is of type ChainRulesCore.ZeroTangent\n\n\n\nprob_node2 = ODEProblem(dudt_node, u0, tspan, res2_node.minimizer)\ns = solve(prob_node2, Tsit5(), saveat = 1)\n\n┌ Warning: sciml_train is being deprecated in favor of direct usage of Optimization.jl. Please consult the Optimization.jl documentation for more details. Optimization.jl's PolyOpt solver is the polyalgorithm of sciml_train\n└ @ DiffEqFlux /Users/stevenchiu/.julia/packages/DiffEqFlux/Em1Aj/src/train.jl:6\n\n\nLoadError: Need an adjoint for constructor SciMLSensitivity.InterpolatingAdjoint{0, true, Val{:central}, SciMLSensitivity.ReverseDiffVJP{false}}. Gradient is of type ChainRulesCore.ZeroTangent\n\n\n\nscatter(solution, vars=[2,3,4], label=[\"True Exposed\" \"True Infected\" \"True Recovered\"])\nplot!(s, vars=[2,3,4], label=[\"Estimated Exposed\" \"Estimated Infected\" \"Estimated Recovered\"])\n\n# Plot the losses\nplot(losses, yaxis = :log, xaxis = :log, xlabel = \"Iterations\", ylabel = \"Loss\")\n\n# Extrapolate out\nprob_node_extrapolate = ODEProblem(dudt_node,u0, tspan2, res2_node.minimizer)\n_sol_node = solve(prob_node_extrapolate, Vern7(), abstol=1e-12, reltol=1e-12, saveat = 1)\np_node = scatter(solution_extrapolate, vars=[2,3,4], legend = :topleft, label=[\"True Exposed\" \"True Infected\" \"True Recovered\"], title=\"Neural ODE Extrapolation\")\nplot!(p_node,_sol_node, lw=5, vars=[2,3,4], label=[\"Estimated Exposed\" \"Estimated Infected\" \"Estimated Recovered\"])\nplot!(p_node,[20.99,21.01],[0.0,maximum(hcat(Array(solution_extrapolate[2:4,:]),Array(_sol_node[2:4,:])))],lw=5,color=:black,label=\"Training Data End\")\n\nsavefig(\"neuralode_extrapolation.png\")\nsavefig(\"neuralode_extrapolation.pdf\")\n\n\n### Universal ODE Part 1\n\nann = FastChain(FastDense(3, 64, tanh),FastDense(64, 64, tanh), FastDense(64, 1))\np = Float64.(initial_params(ann))\n\nfunction dudt_(u,p,t)\n    S,E,I,R,N,D,C = u\n    F, β0,α,κ,μ,σ,γ,d,λ = p_\n    z = ann([S/N,I,D/N],p) # Exposure does not depend on exposed, removed, or cumulative!\n    dS = -β0*S*F/N - z[1] -μ*S # susceptible\n    dE = β0*S*F/N + z[1] -(σ+μ)*E # exposed\n    dI = σ*E - (γ+μ)*I # infected\n    dR = γ*I - μ*R # removed (recovered + dead)\n    dN = -μ*N # total population\n    dD = d*γ*I - λ*D # severe, critical cases, and deaths\n    dC = σ*E # +cumulative cases\n\n    [dS,dE,dI,dR,dN,dD,dC]\nend\nprob_nn = ODEProblem(dudt_,u0, tspan, p)\ns = concrete_solve(prob_nn, Tsit5(), u0, p, saveat = 1)\n\nplot(solution, vars=[2,3,4])\nplot!(s[2:4,:]')\n\nfunction predict(θ)\n    Array(concrete_solve(prob_nn, Vern7(), u0, θ, saveat = solution.t,\n                         abstol=1e-6, reltol=1e-6,\n                         sensealg = InterpolatingAdjoint(autojacvec=ReverseDiffVJP())))\nend\n\n# No regularisation right now\nfunction loss(θ)\n    pred = predict(θ)\n    sum(abs2, noisy_data[2:4,:] .- pred[2:4,:]), pred # + 1e-5*sum(sum.(abs, params(ann)))\nend\n\nloss(p)\n\nconst losses = []\ncallback(θ,l,pred) = begin\n    push!(losses, l)\n    if length(losses)%50==0\n        println(losses[end])\n    end\n    false\nend\n\nres1_uode = DiffEqFlux.sciml_train(loss, p, ADAM(0.01), cb=callback, maxiters = 500)\nres2_uode = DiffEqFlux.sciml_train(loss, res1_uode.minimizer, BFGS(initial_stepnorm=0.01), cb=callback, maxiters = 10000)\n\nloss(res2_uode.minimizer)\n\nprob_nn2 = ODEProblem(dudt_,u0, tspan, res2_uode.minimizer)\nuode_sol = solve(prob_nn2, Tsit5(), saveat = 1)\nplot(solution, vars=[2,3,4])\nplot!(uode_sol, vars=[2,3,4])\n\n# Plot the losses\nplot(losses, yaxis = :log, xaxis = :log, xlabel = \"Iterations\", ylabel = \"Loss\")\n\n# Collect the state trajectory and the derivatives\nX = noisy_data\n# Ideal derivatives\nDX = Array(solution(solution.t, Val{1}))\n\n# Extrapolate out\nprob_nn2 = ODEProblem(dudt_,u0, tspan2, res2_uode.minimizer)\n_sol_uode = solve(prob_nn2, Vern7(), abstol=1e-12, reltol=1e-12, saveat = 1)\np_uode = scatter(solution_extrapolate, vars=[2,3,4], legend = :topleft, label=[\"True Exposed\" \"True Infected\" \"True Recovered\"], title=\"Universal ODE Extrapolation\")\nplot!(p_uode,_sol_uode, lw = 5, vars=[2,3,4], label=[\"Estimated Exposed\" \"Estimated Infected\" \"Estimated Recovered\"])\nplot!(p_uode,[20.99,21.01],[0.0,maximum(hcat(Array(solution_extrapolate[2:4,:]),Array(_sol_uode[2:4,:])))],lw=5,color=:black,label=\"Training Data End\")\n\nsavefig(\"universalode_extrapolation.png\")\nsavefig(\"universalode_extrapolation.pdf\")\n\n### Universal ODE Part 2: SInDy to Equations\n\n# Create a Basis\n@variables u[1:3]\n# Lots of polynomials\npolys = Operation[]\nfor i ∈ 0:2, j ∈ 0:2, k ∈ 0:2\n    push!(polys, u[1]^i * u[2]^j * u[3]^k)\nend\n\n# And some other stuff\nh = [cos.(u)...; sin.(u)...; unique(polys)...]\nbasis = Basis(h, u)\n\nX = noisy_data\n# Ideal derivatives\nDX = Array(solution(solution.t, Val{1}))\nS,E,I,R,N,D,C = eachrow(X)\nF,β0,α,κ,μ,_,γ,d,λ = p_\nL = β.(0:tspan[end],β0,D,N,κ,α).*S.*I./N\nL̂ = vec(ann([S./N I D./N]',res2_uode.minimizer))\nX̂ = [S./N I D./N]'\n\nscatter(L,title=\"Estimated vs Expected Exposure Term\",label=\"True Exposure\")\nplot!(L̂,label=\"Estimated Exposure\")\nsavefig(\"estimated_exposure.png\")\nsavefig(\"estimated_exposure.pdf\")\n\n# Create an optimizer for the SINDY problem\nopt = SR3()\n# Create the thresholds which should be used in the search process\nthresholds = exp10.(-6:0.1:1)\n\n# Test on original data and without further knowledge\nΨ_direct = SInDy(X[2:4, :], DX[2:4, :], basis, thresholds, opt = opt, maxiter = 50000) # Fail\nprintln(Ψ_direct.basis)\n# Test on ideal derivative data ( not available )\nΨ_ideal = SInDy(X[2:4, 5:end], L[5:end], basis, thresholds, opt = opt, maxiter = 50000) # Succeed\nprintln(Ψ_ideal.basis)\n# Test on uode derivative data\nΨ = SInDy(X̂[:, 2:end], L̂[2:end], basis, thresholds,  opt = opt, maxiter = 10000, normalize = true, denoise = true) # Succeed\nprintln(Ψ.basis)\n\n# Build a ODE for the estimated system\nfunction approx(u,p,t)\n    S,E,I,R,N,D,C = u\n    F, β0,α,κ,μ,σ,γ,d,λ = p_\n    z = Ψ([S/N,I,D/N]) # Exposure does not depend on exposed, removed, or cumulative!\n    dS = -β0*S*F/N - z[1] -μ*S # susceptible\n    dE = β0*S*F/N + z[1] -(σ+μ)*E # exposed\n    dI = σ*E - (γ+μ)*I # infected\n    dR = γ*I - μ*R # removed (recovered + dead)\n    dN = -μ*N # total population\n    dD = d*γ*I - λ*D # severe, critical cases, and deaths\n    dC = σ*E # +cumulative cases\n\n    [dS,dE,dI,dR,dN,dD,dC]\nend\n\n# Create the approximated problem and solution\na_prob = ODEProblem{false}(approx, u0, tspan2, p_)\na_solution = solve(a_prob, Tsit5())\n\np_uodesindy = scatter(solution_extrapolate, vars=[2,3,4], legend = :topleft, label=[\"True Exposed\" \"True Infected\" \"True Recovered\"])\nplot!(p_uodesindy,a_solution, lw = 5, vars=[2,3,4], label=[\"Estimated Exposed\" \"Estimated Infected\" \"Estimated Recovered\"])\nplot!(p_uodesindy,[20.99,21.01],[0.0,maximum(hcat(Array(solution_extrapolate[2:4,:]),Array(_sol_uode[2:4,:])))],lw=5,color=:black,label=\"Training Data End\")\n\nsavefig(\"universalodesindy_extrapolation.png\")\nsavefig(\"universalodesindy_extrapolation.pdf\")\n\n\n\n\n\nReferences\n\n[1] Rackauckas, C. et al. 2020. Universal differential equations for scientific machine learning. arXiv preprint arXiv:2001.04385. (2020)."
  },
  {
    "objectID": "intro-julia.html",
    "href": "intro-julia.html",
    "title": "PART I: Intro. to Julia",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "intro-julia.html#outline",
    "href": "intro-julia.html#outline",
    "title": "PART I: Intro. to Julia",
    "section": "Outline",
    "text": "Outline\nThe presentation will give a short tour about essential ideas behind Julia. Though this tutorial put emphasis on the Scientific Machine Learning, Julia is designed for generic purpose"
  },
  {
    "objectID": "intro-julia.html#what-is-julia",
    "href": "intro-julia.html#what-is-julia",
    "title": "PART I: Intro. to Julia",
    "section": "What is Julia?",
    "text": "What is Julia?\nJulia is a high-level programming language that well suited for computational science. It supports Just-in-time (JIT) and multiple dispatch to create static"
  },
  {
    "objectID": "intro-julia.html#why-julia-is-fast",
    "href": "intro-julia.html#why-julia-is-fast",
    "title": "PART I: Intro. to Julia",
    "section": "Why Julia is fast?",
    "text": "Why Julia is fast?"
  },
  {
    "objectID": "intro-julia.html#multiple-dispatch",
    "href": "intro-julia.html#multiple-dispatch",
    "title": "PART I: Intro. to Julia",
    "section": "Multiple Dispatch",
    "text": "Multiple Dispatch\n\nfunction multiply(a,b)\n    return a*b\nend\n\nmultiply (generic function with 1 method)\n\n\n\n@code_llvm multiply(1,2)\n\n;  @ In[47]:1 within `multiply`\ndefine i64 @julia_multiply_2300(i64 signext %0, i64 signext %1) #0 {\ntop:\n;  @ In[47]:2 within `multiply`\n; ┌ @ int.jl:88 within `*`\n   %2 = mul i64 %1, %0\n; └\n  ret i64 %2\n}"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup Julia",
    "section": "",
    "text": "In this section, we are going to setup Julia on your on computer. There are several ways to accomplish this task. You can choose one of the following methods to get Julia and Jupyter installed on your local machine."
  },
  {
    "objectID": "setup.html#install-julia",
    "href": "setup.html#install-julia",
    "title": "Setup Julia",
    "section": "Install Julia",
    "text": "Install Julia\n\nOfficial Julia Release\nThe latest Julia release can be found at official website. Download the installer in respect of your operating system.\n\nDownload Julia installer: https://julialang.org/downloads/\nSpecific instructions for each platform: https://julialang.org/downloads/platform/\n\n\n\njill with pip\nIf you have already installed pip (if not, try install pip here). Jill is a Julia installer for Linux, MacOS and Windows. Jill can be installed via pip. Use the following command to get jill installed\npip install jill\nThen, use jill to install Julia\njill install"
  },
  {
    "objectID": "setup.html#what-is-expected-when-julia-is-successfully-installed",
    "href": "setup.html#what-is-expected-when-julia-is-successfully-installed",
    "title": "Setup Julia",
    "section": "What is expected when Julia is successfully installed?",
    "text": "What is expected when Julia is successfully installed?\nAfter getting Julia installed, click Julia app icon or type julia on cmd/terminal. The Julia REPL1 will show up (Figure 1).\n\n\n\nFigure 1: Julia initiated on the terminal (MacOS)"
  },
  {
    "objectID": "setup.html#sec-ijulia",
    "href": "setup.html#sec-ijulia",
    "title": "Setup Julia",
    "section": "Install IJulia: an Julia package for connecting jupyter",
    "text": "Install IJulia: an Julia package for connecting jupyter\nIJulia is an interface that combines Julia and Jupyter interactive environment. This interface can be installed with Julia package manager — Pkg.jl. The following steps show how to get IJulia installed. Noted that any registered packages2 can be installed via these steps.\n\nInitiate Julia REPL\nInstall IJUlia. The following two methods are equivalent.\n\n\nUse Pkg.addUse Pkg interative environment\n\n\nusing Pkg\nPkg.add(\"IJulia\")\n\n\nType ] (right square bracket on your keyboard) to initiate Pkg environment.\n]\nThe Pkg mode in Julia REPL is a shortcut to manipulate the Julia environment. To futher install the package, type\nadd IJulia\n\n\n\nFigure 2: Installing IJulia. Noted that the Pkg environment is started ((@v1.8) pkg> in blue)\n\n\n\n\n\n\nGet Started with Jupyter and Julia\nOnce IJulia.jl is successfully installed, use the following command to initiate jupyter session (Figure 3).\nusing IJulia\njupyterlab()\n\n\n\nFigure 3: Jupyter Lab. Noted that the Julia kernel is installed.\n\n\n\n\nDone!\nCongratulations! You have successfully get started with the Julia programming language. In the following session, we will go through Julia basics and SciML package. The hands-on sessions will require to use JupyterLab to run Julia sessions. The source code of hands-on session can be downloaded here or view on this website.\nFor further information, the Appendix includes advanced topics such as developer setup and other alternative platforms."
  }
]