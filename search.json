[
  {
    "objectID": "index_sciml-intro.html",
    "href": "index_sciml-intro.html",
    "title": "Part II: SciML Packages",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Session Info",
    "section": "",
    "text": "Place: Blocker 220, Texas A&M University (Map)\nTime: 10:00 am - 12:00 pm. October 25, 2022\n(Latest information is on TAMIDS page)"
  },
  {
    "objectID": "index.html#presenter",
    "href": "index.html#presenter",
    "title": "Session Info",
    "section": "Presenter",
    "text": "Presenter\n\nSteven Shao-Ting Chiu — Ph.D. Student, Department of Electrical Engineering, Texas A&M University\nAdvisor: Dr. Ulisses Braga-Neto"
  },
  {
    "objectID": "index.html#background-and-objectives",
    "href": "index.html#background-and-objectives",
    "title": "Session Info",
    "section": "Background and Objectives",
    "text": "Background and Objectives\nJulia (https://julialang.org/) is a generic programming language designed for high-performance computing. It solves the “two language problem” that typically occurs in scientific computing. Julia is dynamically typed like scripting language such as Python and can be compiled into native machine code. Besides, composablility via multiple dispatches makes Julia works well on the integration across packages. SciML (https://sciml.ai/) is an open-source software for scientific machine learning based on the Julia language that combines machine learning and scientific computing by integrating numerous standalone packages. Notebly, Julia is an open-source project under an MIT license.\nThis worhshop aims to introduce the potential of the Scientific Machine Learning field with Julia programming language. First, we will give an introductory overview of the Julia programming language and explore the Julia SciML ecosystem as an example of its application.\nBoth sessions will include presentations and hands-on sessions. Prior knowledge of Python is recommended, and participants are encouraged to bring their own laptops."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Session Info",
    "section": "Schedule",
    "text": "Schedule\n\nSchedule (Total: 2hrs)\n\n\nTime\nContent\n\n\n\n\n25 min\nIntroduction to the Julia Programming Language\n\n\n25 min\nThe Julia SciML Ecosystem\n\n\n10 min\nBreak\n\n\n30 min\nHands-on session with nerual differential equation\n\n\n30 min\nHands-on session with SciML application"
  },
  {
    "objectID": "index.html#access-to-this-webpage",
    "href": "index.html#access-to-this-webpage",
    "title": "Session Info",
    "section": "Access to this webpage",
    "text": "Access to this webpage\n\n\n\nFigure 1: https://stevengogogo.github.io/Julia-for-SciML\n\n\n\nHands-on session\n\n\n\n\n\n\nDownload hands-on tutorials (Repo, zip)\nView on"
  },
  {
    "objectID": "SLIDE_julia-intro.html#session",
    "href": "SLIDE_julia-intro.html#session",
    "title": "Introduction to Julia Language",
    "section": "Session",
    "text": "Session\n\n\n\n\n\nTime\nContent\n\n\n\n\n25 min\nIntroduction to the Julia Programming Language\n\n\n25 min\nThe Julia SciML Ecosystem\n\n\n10 min\nBreak\n\n\n30 min\nHands-on session with nerual differential equation\n\n\n30 min\nHands-on session with SciML application\n\n\n\n\n\n\n\nFigure 1: Tutorial Website1\n\n\n\n\nhttps://stevengogogo.github.io/Julia-for-SciML"
  },
  {
    "objectID": "SLIDE_julia-intro.html#what-is-julia",
    "href": "SLIDE_julia-intro.html#what-is-julia",
    "title": "Introduction to Julia Language",
    "section": "What is Julia?",
    "text": "What is Julia?\n\n\n\nJulia is a generic high-performance programming language1\n\nJust-In-Time (JIT) compilation\nMultiple dispatch creates type-stability\n\nVia multiple dispatch, different programming patterns can be adapted to the application.\nJulia supports high-level syntax that is approachable."
  },
  {
    "objectID": "SLIDE_julia-intro.html#a-quick-look-at-multiple-dispatch",
    "href": "SLIDE_julia-intro.html#a-quick-look-at-multiple-dispatch",
    "title": "Introduction to Julia Language",
    "section": "A quick look at multiple dispatch",
    "text": "A quick look at multiple dispatch\nA Generic function\njulia> function multiply(a,b)\n           return a*b\n       end\nmultiply (generic function with 1 method)"
  },
  {
    "objectID": "SLIDE_julia-intro.html#a-quick-look-at-multiple-dispatch-1",
    "href": "SLIDE_julia-intro.html#a-quick-look-at-multiple-dispatch-1",
    "title": "Introduction to Julia Language",
    "section": "A quick look at multiple dispatch",
    "text": "A quick look at multiple dispatch\n\nType inference in Julia solves the optimized data type2.\n\nInteger Input\njulia> @code_llvm multiply(1, 1)\n;  @ REPL[1]:1 within `multiply`\ndefine i64 @julia_multiply_660(i64 signext %0, i64 signext %1) #0 {\ntop:\n;  @ REPL[1]:2 within `multiply`\n; ┌ @ int.jl:88 within `*`\n   %2 = mul i64 %1, %0\n; └\n  ret i64 %2\n\n@code_llvm shows the processed script in intermediate presentation (IR).\n\n\nThis is my note.\n\nIt can contain Markdown\nlike this list"
  },
  {
    "objectID": "SLIDE_julia-intro.html#a-quick-look-at-multiple-dispatch-2",
    "href": "SLIDE_julia-intro.html#a-quick-look-at-multiple-dispatch-2",
    "title": "Introduction to Julia Language",
    "section": "A quick look at multiple dispatch",
    "text": "A quick look at multiple dispatch\n\njulia> @code_llvm multiply(2.0,2)\n;  @ REPL[1]:1 within `multiply`\ndefine double @julia_multiply_208(double %0, i64 signext %1) #0 {\ntop:\n;  @ REPL[1]:2 within `multiply`\n; ┌ @ promotion.jl:389 within `*`\n; │┌ @ promotion.jl:359 within `promote`\n; ││┌ @ promotion.jl:336 within `_promote`\n; │││┌ @ number.jl:7 within `convert`\n; ││││┌ @ float.jl:146 within `Float64`\n       %2 = sitofp i64 %1 to double\n# Skip\n  ret double %3\n}"
  },
  {
    "objectID": "SLIDE_julia-intro.html#significant-features",
    "href": "SLIDE_julia-intro.html#significant-features",
    "title": "Introduction to Julia Language",
    "section": "Significant Features",
    "text": "Significant Features\n\nJulia Base and standard library are written in Julia itself.\n\n\n\nJulia is always Just-in-Time (JIT) compiled\n\n\n\n\nMultiple dispatch allows many conbinatoins of argument types\n\nApproaching the speed of statically-compiled language like C/Fortran3\n\n\n\n\nWhy Julia is fast?4\n\nJIT\nType inference\nType specialization in functions"
  },
  {
    "objectID": "SLIDE_julia-intro.html#ecosystem",
    "href": "SLIDE_julia-intro.html#ecosystem",
    "title": "Introduction to Julia Language",
    "section": "Ecosystem",
    "text": "Ecosystem"
  },
  {
    "objectID": "SLIDE_julia-intro.html#references",
    "href": "SLIDE_julia-intro.html#references",
    "title": "Introduction to Julia Language",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n1. Bezanson, J., Edelman, A., Karpinski, S., and Shah, V.B. (2017). Julia: A fresh approach to numerical computing. SIAM review 59, 65–98.\n\n\n2. Nash, J. (2016). Inference Convergence Algorithm in Julia - Julia Computing.\n\n\n3. Julia Documentation \\(\\cdot\\) The Julia Language.\n\n\n4. Rackauckas, C. (2022). Parallel computing and scientific machine learning (SciML): Methods and applications 10.5281/zenodo.6917234."
  },
  {
    "objectID": "hands-on/julia_basics.html",
    "href": "hands-on/julia_basics.html",
    "title": "Julia Basics",
    "section": "",
    "text": "[1]\n\nusing Pkg\nPkg.activate(\"tutorial\")\n\n\n?+\n\nsearch: +\n\n\n\n+(x, y...)\nAddition operator. x+y+z+... calls this function with all arguments, i.e. +(x, y, z, ...).\n\nExamples\njulia> 1 + 20 + 4\n25\n\njulia> +(1, 20, 4)\n25\n\ndt::Date + t::Time -> DateTime\nThe addition of a Date with a Time produces a DateTime. The hour, minute, second, and millisecond parts of the Time are used along with the year, month, and day of the Date to create the new DateTime. Non-zero microseconds or nanoseconds in the Time type will result in an InexactError being thrown.\n\n\n\n\n\n\n\nReferences\n\n[1] Rackauckas, C. A Deep Introduction to Julia for Data Science and Scientific Computing. http://ucidatascienceinitiative.github.io/IntroToJulia/."
  },
  {
    "objectID": "hands-on/lotka.html",
    "href": "hands-on/lotka.html",
    "title": "TAMIDS SciML Lab Workshop: Julia Softwares for Scientific Machine Learning",
    "section": "",
    "text": "Binder\n\n\nSample code is modified from ChrisRackauckas/universsal_differential_equations. This is the part of the work of\n\nRackauckas, Christopher, et al. “Universal differential equations for scientific machine learning.” arXiv preprint arXiv:2001.04385 (2020).\n\n\n\n\n## Environment and packages\ncd(@__DIR__)\nusing Pkg\nPkg.activate(\"lotka\")\n\n\nPkg.instantiate() # This step will take a while for numerous packages\n\n\n\n\n\nWhen importing multiple packages, this requires precompilation that takes some time.\n\n\nusing OrdinaryDiffEq\nusing ModelingToolkit\nusing DataDrivenDiffEq\nusing LinearAlgebra, ComponentArrays\nusing Optimization, OptimizationOptimisers, OptimizationOptimJL #OptimizationFlux for ADAM and OptimizationOptimJL for BFGS\nusing DiffEqSensitivity\nusing Lux\nusing Plots\ngr()\nusing Statistics\n\n# Set a random seed for reproduceable behaviour\nusing Random\nrng = Random.default_rng()\nRandom.seed!(1234);\n\n\n\n\nFor simplicity, we use Lotka-Volterra system as an example\n\\[\\begin{align}\n    \\dot{x} &= \\alpha x - \\beta xy\\\\\n    \\dot{y} &= \\gamma xy- \\delta y\n\\end{align}\\]\nwhere \\(\\alpha, \\beta, \\gamma\\), and \\(\\delta\\) are positive real parameters\n\n## Data generation\nfunction lotka!(du, u, p, t)\n    α, β, γ, δ = p\n    du[1] = α*u[1] - β*u[2]*u[1]\n    du[2] = γ*u[1]*u[2]  - δ*u[2]\nend\n\nlotka! (generic function with 1 method)\n\n\n\n# Define the experimental parameter\ntspan = (0.0,3.0)\nu0 = [0.44249296,4.6280594]\np_ = [1.3, 0.9, 0.8, 1.8]\n\n4-element Vector{Float64}:\n 1.3\n 0.9\n 0.8\n 1.8\n\n\n\n\n\nVern7 is used for non-stiff problems. Numerous solvers can be found on DifferentialEquations.jl’s doc\n\n\n# Solve\nprob = ODEProblem(lotka!, u0,tspan, p_)\nsolution = solve(prob, Vern7(), abstol=1e-12, reltol=1e-12, saveat = 0.1)\n\nretcode: Success\nInterpolation: 1st order linear\nt: 31-element Vector{Float64}:\n 0.0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1.0\n 1.1\n 1.2\n ⋮\n 1.9\n 2.0\n 2.1\n 2.2\n 2.3\n 2.4\n 2.5\n 2.6\n 2.7\n 2.8\n 2.9\n 3.0\nu: 31-element Vector{Vector{Float64}}:\n [0.44249296, 4.6280594]\n [0.34212452862086234, 3.98764547181634]\n [0.2793966078254349, 3.4139529441083147]\n [0.2394952228707143, 2.9110318130603883]\n [0.21413620714095402, 2.4758280205419836]\n [0.19854852659179129, 2.1022922430734137]\n [0.18991187927524103, 1.7834096349202704]\n [0.18652973211225643, 1.5121821427640152]\n [0.18737918127509637, 1.2820806846455604]\n [0.1918587411736629, 1.087227597605956]\n [0.1996432344128222, 0.9224424008592909]\n [0.2105985019620811, 0.7832199752377471]\n [0.22473063540355143, 0.6656774980182895]\n ⋮\n [0.4333056937367298, 0.22471175932636067]\n [0.48425346211989406, 0.1947029152564331]\n [0.5425361548950363, 0.16943926722620506]\n [0.6091040110729008, 0.14819092695665834]\n [0.6850407509453579, 0.13034710141497852]\n [0.7715795653361799, 0.11539841080610512]\n [0.8701212001899306, 0.10292221843899205]\n [0.9822541897624152, 0.09257065810821445]\n [1.1097772412678872, 0.08406114122763123]\n [1.2547236687788759, 0.07716924328704818]\n [1.419387582491876, 0.07172402271816045]\n [1.606351205697802, 0.06760604257226555]\n\n\n\n\n\n\n\n# Ideal data\nX = Array(solution)\nt = solution.t\nDX = Array(solution(solution.t, Val{1}))\n\nfull_problem = DataDrivenProblem(X, t = t, DX = DX)\n\n# Add noise in terms of the mean\nx̄ = mean(X, dims = 2)\nnoise_magnitude = 5e-3\nXₙ = X .+ (noise_magnitude*x̄) .* randn(eltype(X), size(X))\n\n2×31 Matrix{Float64}:\n 0.442554  0.346422  0.283868  0.239053  …  1.25327    1.42578    1.60411\n 4.62367   3.98581   3.41621   2.91364      0.0747481  0.0616359  0.0619753\n\n\n\n\n\n\nplot(solution, alpha = 0.75, color = :black, label = [\"True Data\" nothing])\nscatter!(t, transpose(Xₙ), color = :red, label = [\"Noisy Data\" nothing])\n\n\n\n\n\n\n\nSuppose we only know part of the Lotka-Voltera model, and use CNN to surrogate the unknown part\n\\[\\begin{align}\n    \\dot{x} &= \\theta_1 x + U_1(\\theta_3, x, y)\\\\\n    \\dot{y} &= -\\theta_2 y + U_2(\\theta_3..., x, y)\n\\end{align}\\]\n\n## Define the network\n# Gaussian RBF as activation\nrbf(x) = exp.(-(x.^2))\n\n# Multilayer FeedForward\nU = Lux.Chain(\n    Lux.Dense(2,5,rbf), Lux.Dense(5,5, rbf), Lux.Dense(5,5, rbf), Lux.Dense(5,2)\n)\n# Get the initial parameters and state variables of the model\np, st = Lux.setup(rng, U)\n\n# Define the hybrid model\nfunction ude_dynamics!(du,u, p, t, p_true)\n    û = U(u, p, st)[1] # Network prediction\n    du[1] = p_true[1]*u[1] + û[1]\n    du[2] = -p_true[4]*u[2] + û[2]\nend\n\nude_dynamics! (generic function with 1 method)\n\n\n\n# Closure with the known parameter\nnn_dynamics!(du,u,p,t) = ude_dynamics!(du,u,p,t,p_)\n# Define the problem (Fix: https://discourse.julialang.org/t/issue-with-ude-repository-lv-scenario-1/88618/5)\nprob_nn = ODEProblem{true, SciMLBase.FullSpecialize}(nn_dynamics!,Xₙ[:, 1], tspan, p)\n\n\nODEProblem with uType Vector{Float64} and tType Float64. In-place: true\ntimespan: (0.0, 3.0)\nu0: 2-element Vector{Float64}:\n 0.4425543496585194\n 4.6236666047953\n\n\n\n\n\n\n\n## Function to train the network\n# Define a predictor\nfunction predict(θ, X = Xₙ[:,1], T = t)\n    _prob = remake(prob_nn, u0 = X, tspan = (T[1], T[end]), p = θ)\n    Array(solve(_prob, Vern7(), saveat = T,\n                abstol=1e-6, reltol=1e-6,\n                sensealg = ForwardDiffSensitivity()\n                ))\nend\n\n# Simple L2 loss\nfunction loss(θ)\n    X̂ = predict(θ)\n    sum(abs2, Xₙ .- X̂)\nend\n\n# Container to track the losses\nlosses = Float64[]\n\ncallback = function (p, l)\n  push!(losses, l)\n  if length(losses)%50==0\n      println(\"Current loss after $(length(losses)) iterations: $(losses[end])\")\n  end\n  return false\nend\n\n#19 (generic function with 1 method)\n\n\n\n\n\nThe training is splitted to two steps: 1. ADAM: for better convergence 2. BFGS: get better position\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, ComponentVector{Float64}(p))\nres1 = Optimization.solve(optprob, ADAM(0.1), callback=callback, maxiters = 200)\n\n@info \"Training loss after $(length(losses)) iterations: $(losses[end])\"\n# Train with BFGS\n@time optprob2 = Optimization.OptimizationProblem(optf, res1.minimizer)\n@time res2 = Optimization.solve(optprob2, Optim.BFGS(initial_stepnorm=0.01), callback=callback, maxiters = 3000)\n@info \"Final training loss after $(length(losses)) iterations: $(losses[end])\"\n\nCurrent loss after 50 iterations: 3.469528000842328\nCurrent loss after 100 iterations: 1.6085778580005679\nCurrent loss after 150 iterations: 1.0932233615139908\nCurrent loss after 200 iterations: 0.9008213085586918\n  0.002767 seconds (912 allocations: 55.714 KiB, 97.56% compilation time)\n\n\n┌ Info: Training loss after 201 iterations: 0.9008213085586918\n└ @ Main In[104]:6\n\n\nCurrent loss after 250 iterations: 0.00835701277177789\nCurrent loss after 300 iterations: 0.0012008428507987853\nCurrent loss after 350 iterations: 0.0007589772027163713\nCurrent loss after 400 iterations: 0.0007090003812023018\nCurrent loss after 450 iterations: 0.0006963845360709188\nCurrent loss after 500 iterations: 0.0006879639315406285\nCurrent loss after 550 iterations: 0.0006737836722368439\nCurrent loss after 600 iterations: 0.0006610212120100604\nCurrent loss after 650 iterations: 0.0006304002713236008\nCurrent loss after 700 iterations: 0.0005887565584171636\nCurrent loss after 750 iterations: 0.0005830120824833022\nCurrent loss after 800 iterations: 0.000567571010413902\nCurrent loss after 850 iterations: 0.0005601012883909123\nCurrent loss after 900 iterations: 0.0005481287746361439\nCurrent loss after 950 iterations: 0.0005426657868937582\nCurrent loss after 1000 iterations: 0.0005389589538206719\nCurrent loss after 1050 iterations: 0.0005378070015787143\nCurrent loss after 1100 iterations: 0.00053260691150804\nCurrent loss after 1150 iterations: 0.0005247085298999025\nCurrent loss after 1200 iterations: 0.0005222863437639172\nCurrent loss after 1250 iterations: 0.000514100635755448\nCurrent loss after 1300 iterations: 0.0005109459260636017\nCurrent loss after 1350 iterations: 0.0005079065846714043\nCurrent loss after 1400 iterations: 0.0005033748616302439\nCurrent loss after 1450 iterations: 0.0004994904499349531\nCurrent loss after 1500 iterations: 0.0004966601640042935\nCurrent loss after 1550 iterations: 0.0004923552185348715\nCurrent loss after 1600 iterations: 0.0004889445340668124\nCurrent loss after 1650 iterations: 0.0004805067981551289\nCurrent loss after 1700 iterations: 0.0004766270876236727\nCurrent loss after 1750 iterations: 0.0004669801522855263\nCurrent loss after 1800 iterations: 0.00046399927019063553\nCurrent loss after 1850 iterations: 0.00045877442467472985\nCurrent loss after 1900 iterations: 0.0004531416076322169\nCurrent loss after 1950 iterations: 0.00045018827875346385\nCurrent loss after 2000 iterations: 0.00044866125441227667\nCurrent loss after 2050 iterations: 0.0004385317791146122\nCurrent loss after 2100 iterations: 0.00043123783494311606\nCurrent loss after 2150 iterations: 0.00043041897973958385\nCurrent loss after 2200 iterations: 0.0004284921491458308\nCurrent loss after 2250 iterations: 0.0004278384777668959\nCurrent loss after 2300 iterations: 0.0004242955366192935\nCurrent loss after 2350 iterations: 0.0004233850019719135\nCurrent loss after 2400 iterations: 0.00042183393350811633\nCurrent loss after 2450 iterations: 0.0004212106099017218\nCurrent loss after 2500 iterations: 0.0004190375099401659\nCurrent loss after 2550 iterations: 0.00041641136591938967\nCurrent loss after 2600 iterations: 0.00041338080498632866\nCurrent loss after 2650 iterations: 0.00041134025280670385\nCurrent loss after 2700 iterations: 0.00040197161275078435\nCurrent loss after 2750 iterations: 0.00039949066296698606\nCurrent loss after 2800 iterations: 0.000393878651282027\nCurrent loss after 2850 iterations: 0.00039216371202708785\nCurrent loss after 2900 iterations: 0.0003874546834286143\nCurrent loss after 2950 iterations: 0.00037598086076886123\nCurrent loss after 3000 iterations: 0.0003737354834934274\nCurrent loss after 3050 iterations: 0.00037237715575335565\nCurrent loss after 3100 iterations: 0.000367860777556065\nCurrent loss after 3150 iterations: 0.0003606938309170762\nCurrent loss after 3200 iterations: 0.0003567357981520652\n 73.392067 seconds (623.24 M allocations: 115.171 GiB, 15.35% gc time, 0.22% compilation time)\n\n\n┌ Info: Final training loss after 3202 iterations: 0.0003567221878942813\n└ @ Main In[104]:10\n\n\n\n\n\n\n# Plot the losses\npl_losses = plot(1:200, losses[1:200], yaxis = :log10, xaxis = :log10, xlabel = \"Iterations\", ylabel = \"Loss\", label = \"ADAM\", color = :blue)\nplot!(201:length(losses), losses[201:end], yaxis = :log10, xaxis = :log10, xlabel = \"Iterations\", ylabel = \"Loss\", label = \"BFGS\", color = :red)\n\n\n\n\n\n# Rename the best candidate\np_trained = res2.minimizer;\n\n\n## Analysis of the trained network\n# Plot the data and the approximation\nts = first(solution.t):mean(diff(solution.t))/2:last(solution.t)\nX̂ = predict(p_trained, Xₙ[:,1], ts)\n# Trained on noisy data vs real solution\npl_trajectory = plot(ts, transpose(X̂), xlabel = \"t\", ylabel =\"x(t), y(t)\", color = :red, label = [\"UDE Approximation\" nothing])\nscatter!(solution.t, transpose(Xₙ), color = :black, label = [\"Measurements\" nothing])\n\n# Ideal unknown interactions of the predictor\nȲ = [-p_[2]*(X̂[1,:].*X̂[2,:])';p_[3]*(X̂[1,:].*X̂[2,:])']\n# Neural network guess\nŶ = U(X̂,p_trained,st)[1]\n\npl_reconstruction = plot(ts, transpose(Ŷ), xlabel = \"t\", ylabel =\"U(x,y)\", color = :red, label = [\"UDE Approximation\" nothing])\nplot!(ts, transpose(Ȳ), color = :black, label = [\"True Interaction\" nothing])\n\n# Plot the error\npl_reconstruction_error = plot(ts, norm.(eachcol(Ȳ-Ŷ)), yaxis = :log, xlabel = \"t\", ylabel = \"L2-Error\", label = nothing, color = :red)\npl_missing = plot(pl_reconstruction, pl_reconstruction_error, layout = (2,1))\n\npl_overall = plot(pl_trajectory, pl_missing)\n## Symbolic regression via sparse regression ( SINDy based )\n\n\n\n\n\n\n\n\n# Create a Basis\n@variables u[1:2]\n# Generate the basis functions, multivariate polynomials up to deg 5\n# and sine\nb = [polynomial_basis(u, 5); sin.(u)]\nbasis = Basis(b,u);\n\n# Create the thresholds which should be used in the search process\nλ = exp10.(-3:0.01:5)\n# Create an optimizer for the SINDy problem\nopt = STLSQ(λ)\n\n# Define different problems for the recovery\nideal_problem = DirectDataDrivenProblem(X̂, Ȳ)\nnn_problem = DirectDataDrivenProblem(X̂, Ŷ)\n\n# Test on ideal derivative data for unknown function ( not available )\nprintln(\"Sparse regression\")\nfull_res = solve(full_problem, basis, opt, maxiter = 10000, progress = true)\nideal_res = solve(ideal_problem, basis, opt, maxiter = 10000, progress = true)\nnn_res = solve(nn_problem, basis, opt, maxiter = 10000, progress = true, sampler = DataSampler(Batcher(n = 4, shuffle = true)))\n\n\n# Store the results\nresults = [full_res; ideal_res; nn_res]\n# Show the results\nmap(println, results)\n# Show the results\nmap(println ∘ result, results)\n# Show the identified parameters\nmap(println ∘ parameter_map, results)\n\n# Define the recovered, hyrid model\nfunction recovered_dynamics!(du,u, p, t)\n    û = nn_res(u, p) # Network prediction\n    du[1] = p_[1]*u[1] + û[1]\n    du[2] = -p_[4]*u[2] + û[2]\nend\n\n\nestimation_prob = ODEProblem(recovered_dynamics!, u0, tspan, parameters(nn_res))\nestimate = solve(estimation_prob, Tsit5(), saveat = solution.t)\n\nSparse regression\n\n\nSTLSQ  13%|█████▊                                        |  ETA: 0:00:01\n  Threshold:          0.01\n  Best Objective:     0.0\n  Best Sparsity:      23.0\n  Current Objective:  0.0\n  Current Sparsity:   23.0\n\n\nLinear Solution with 2 equations and 20 parameters.\nReturncode: solved\nL₂ Norm error : [31.995291148539735, 1.2046710278865183]\nAIC : [147.43325095127574, 45.77240223675897]\nR² : [-1.2429510122420595, 0.990118386736816]\n\nLinear Solution with 2 equations and 2 parameters.\nReturncode: solved\nL₂ Norm error : [1.3211879418496438e-31, 2.3688938315963e-32]\nAIC : [-4333.198002294267, -4438.038291907829]\nR² : [1.0, 1.0]\n\nLinear Solution with 2 equations and 12 parameters.\nReturncode: solved\nL₂ Norm error : [2.3343801269794584, 0.44274177996973096]\nAIC : [75.71252952361633, -25.70088266885905]\nR² : [0.664896276909329, 0.9252179602574171]\n\nModel ##Basis#1046 with 2 equations\nStates : u[1] u[2]\nParameters : 20\nIndependent variable: t\nEquations\nDifferential(t)(u[1]) = p₁ + p₁₀*(u[2]^2) + p₃*(u[1]^2) + p₁₇*sin(u[1]) + p₂*u[1] + p₄*(u[1]^3) + p₅*u[2] + p₁₂*(u[1]^2)*(u[2]^2) + p₁₅*(u[1]^2)*(u[2]^3) + p₁₃*(u[1]^3)*(u[2]^2) + p₁₁*(u[2]^2)*u[1] + p₁₄*(u[2]^3)*u[1] + p₇*(u[1]^2)*u[2] + p₈*(u[1]^3)*u[2] + p₉*(u[1]^4)*u[2] + p₁₆*(u[2]^4)*u[1] + p₆*u[1]*u[2]\nDifferential(t)(u[2]) = p₁₉*(u[1]^2)*u[2] + p₂₀*(u[1]^3)*u[2] + p₁₈*u[1]*u[2]\nModel ##Basis#1049 with 2 equations\nStates : u[1] u[2]\nParameters : p₁ p₂\nIndependent variable: t\nEquations\nφ₁ = p₁*u[1]*u[2]\nφ₂ = p₂*u[1]*u[2]\nModel ##Basis#1052 with 2 equations\nStates : u[1] u[2]\nParameters : 12\nIndependent variable: t\nEquations\nφ₁ = p₁*(u[1]^3)*u[2] + p₂*(u[1]^4)*u[2]\nφ₂ = p₃*u[1] + p₄*(u[1]^2) + p₅*(u[1]^3) + p₁₀*(u[1]^2)*(u[2]^2) + p₁₁*(u[1]^3)*(u[2]^2) + p₁₂*(u[1]^2)*(u[2]^3) + p₆*u[1]*u[2] + p₇*(u[1]^2)*u[2] + p₈*(u[1]^3)*u[2] + p₉*(u[1]^4)*u[2]\nPair{Sym{Real, Base.ImmutableDict{DataType, Any}}, Float64}[p₁ => 88.0, p₂ => 90.1, p₃ => 45.4, p₄ => 27.6, p₅ => 73.5, p₆ => -1107.4, p₇ => -2835.6, p₈ => 27.9, p₉ => 25.07, p₁₀ => 16.9, p₁₁ => -472.5, p₁₂ => 6115.8, p₁₃ => -117.4, p₁₄ => 22.016, p₁₅ => -659.7, p₁₆ => -25.4, p₁₇ => 62.5, p₁₈ => -13.9, p₁₉ => 31.25, p₂₀ => -15.5]\nPair{Sym{Real, Base.ImmutableDict{DataType, Any}}, Float64}[p₁ => -0.9, p₂ => 0.8]\nPair{Sym{Real, Base.ImmutableDict{DataType, Any}}, Float64}[p₁ => -6.6, p₂ => 4.4, p₃ => -9.3, p₄ => 16.9, p₅ => -5.7, p₆ => -6.9, p₇ => 137.21, p₈ => -221.24, p₉ => 73.6, p₁₀ => -31.08, p₁₁ => 8.16, p₁₂ => 4.3]\n\n\nretcode: Success\nInterpolation: 1st order linear\nt: 31-element Vector{Float64}:\n 0.0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1.0\n 1.1\n 1.2\n ⋮\n 1.9\n 2.0\n 2.1\n 2.2\n 2.3\n 2.4\n 2.5\n 2.6\n 2.7\n 2.8\n 2.9\n 3.0\nu: 31-element Vector{Vector{Float64}}:\n [0.44249296, 4.6280594]\n [0.3627445443308102, 3.9587001242732978]\n [0.3314229485874231, 3.328966119978393]\n [0.3195350372465093, 2.8042757858839753]\n [0.31754887251689373, 2.3961622087687]\n [0.32133497307349757, 2.086000709549741]\n [0.3287978586587243, 1.8510046953171453]\n [0.33869695698345803, 1.6720597813273184]\n [0.3502506915180724, 1.5347311764677078]\n [0.3628864692896785, 1.4280447919046295]\n [0.37619150965147025, 1.3438451858547527]\n [0.38985713275559497, 1.2758010766237882]\n [0.40364237522526314, 1.2189050380292614]\n ⋮\n [0.50077433947757, 0.9091265272622229]\n [0.5161210861840806, 0.8610678370503931]\n [0.5325464705273117, 0.8093005948494036]\n [0.5505276051187781, 0.7536816501146165]\n [0.5707004168174493, 0.6938255066760479]\n [0.5938195858778759, 0.6284921408718404]\n [0.6207598616811292, 0.5555985753475307]\n [0.6529401370168936, 0.4743643719394661]\n [0.6927897952183001, 0.3873020930578921]\n [0.7427774836845894, 0.29576122236710667]\n [0.8063349462336009, 0.20892749866534546]\n [0.8863231701922465, 0.14122793717576548]\n\n\n\n\n\n\n# Plot\nplot(solution)\nplot!(estimate)\n\n## Simulation\n\n# Look at long term prediction\nt_long = (0.0, 50.0)\nestimation_prob = ODEProblem(recovered_dynamics!, u0, t_long, parameters(nn_res))\nestimate_long = solve(estimation_prob, Tsit5()) # Using higher tolerances here results in exit of julia\nplot(estimate_long)\n\ntrue_prob = ODEProblem(lotka!, u0, t_long, p_)\ntrue_solution_long = solve(true_prob, Tsit5(), saveat = estimate_long.t)\nplot!(true_solution_long)\n\n\n\n## Post Processing and Plots\n\nc1 = 3 # RGBA(174/255,192/255,201/255,1) # Maroon\nc2 = :orange # RGBA(132/255,159/255,173/255,1) # Red\nc3 = :blue # RGBA(255/255,90/255,0,1) # Orange\nc4 = :purple # RGBA(153/255,50/255,204/255,1) # Purple\n\np1 = plot(t,abs.(Array(solution) .- estimate)' .+ eps(Float32),\n          lw = 3, yaxis = :log, title = \"Timeseries of UODE Error\",\n          color = [3 :orange], xlabel = \"t\",\n          label = [\"x(t)\" \"y(t)\"],\n          titlefont = \"Helvetica\", legendfont = \"Helvetica\",\n          legend = :topright)\n\n# Plot L₂\np2 = plot3d(X̂[1,:], X̂[2,:], Ŷ[2,:], lw = 3,\n     title = \"Neural Network Fit of U2(t)\", color = c1,\n     label = \"Neural Network\", xaxis = \"x\", yaxis=\"y\",\n     titlefont = \"Helvetica\", legendfont = \"Helvetica\",\n     legend = :bottomright)\nplot!(X̂[1,:], X̂[2,:], Ȳ[2,:], lw = 3, label = \"True Missing Term\", color=c2)\n\np3 = scatter(solution, color = [c1 c2], label = [\"x data\" \"y data\"],\n             title = \"Extrapolated Fit From Short Training Data\",\n             titlefont = \"Helvetica\", legendfont = \"Helvetica\",\n             markersize = 5)\n\nplot!(p3,true_solution_long, color = [c1 c2], linestyle = :dot, lw=5, label = [\"True x(t)\" \"True y(t)\"])\nplot!(p3,estimate_long, color = [c3 c4], lw=1, label = [\"Estimated x(t)\" \"Estimated y(t)\"])\nplot!(p3,[2.99,3.01],[0.0,10.0],lw=1,color=:black, label = nothing)\nannotate!([(1.5,13,text(\"Training \\nData\", 10, :center, :top, :black, \"Helvetica\"))])\nl = @layout [grid(1,2)\n             grid(1,1)]\nplot(p1,p2,p3,layout = l)\n\n┌ Warning: dt(7.105427357601002e-15) <= dtmin(7.105427357601002e-15) at t=3.7372345567423673. Aborting. There is either an error in your model specification or the true solution is unstable.\n└ @ SciMLBase /Users/stevenchiu/.julia/packages/SciMLBase/ZGRni/src/integrator_interface.jl:516"
  },
  {
    "objectID": "hands-on/stack/infectious_model.html",
    "href": "hands-on/stack/infectious_model.html",
    "title": "Example: Infectious Model",
    "section": "",
    "text": "cd(@__DIR__)\nusing Pkg\nPkg.activate(\"tutorial\")\nPkg.instantiate()\n\n  Activating project at `~/Documents/GitHub/Julia-for-SciML/hands-on/tutorial`\n\n\n\nusing DifferentialEquations\nusing ModelingToolkit\nusing DataDrivenDiffEq\nusing LinearAlgebra, DiffEqSensitivity, Optim\nusing DiffEqFlux, Flux\nusing Plots\nusing Optimization\nimport DiffEqSensitivity as ds\nimport DiffEqFlux as df\nimport Optimization as op\ngr();\n\n\nfunction corona!(du,u,p,t)\n    S,E,I,R,N,D,C = u\n    F, β0,α,κ,μ,σ,γ,d,λ = p\n    dS = -β0*S*F/N - β(t,β0,D,N,κ,α)*S*I/N -μ*S # susceptible\n    dE = β0*S*F/N + β(t,β0,D,N,κ,α)*S*I/N -(σ+μ)*E # exposed\n    dI = σ*E - (γ+μ)*I # infected\n    dR = γ*I - μ*R # removed (recovered + dead)\n    dN = -μ*N # total population\n    dD = d*γ*I - λ*D # severe, critical cases, and deaths\n    dC = σ*E # +cumulative cases\n\n    du[1] = dS; du[2] = dE; du[3] = dI; du[4] = dR\n    du[5] = dN; du[6] = dD; du[7] = dC\nend\nβ(t,β0,D,N,κ,α) = β0*(1-α)*(1-D/N)^κ\nS0 = 14e6\nu0 = [0.9*S0, 0.0, 0.0, 0.0, S0, 0.0, 0.0]\np_ = [10.0, 0.5944, 0.4239, 1117.3, 0.02, 1/3, 1/5,0.2, 1/11.2]\nR0 = p_[2]/p_[7]*p_[6]/(p_[6]+p_[5])\ntspan = (0.0, 21.0)\nprob = ODEProblem(corona!, u0, tspan, p_)\nsolution = solve(prob, Vern7(), abstol=1e-12, reltol=1e-12, saveat = 1)\n\ntspan2 = (0.0,60.0)\nprob = ODEProblem(corona!, u0, tspan2, p_)\nsolution_extrapolate = solve(prob, Vern7(), abstol=1e-12, reltol=1e-12, saveat = 1)\n\n# Ideal data\ntsdata = Array(solution)\n# Add noise to the data\nnoisy_data = tsdata + Float32(1e-5)*randn(eltype(tsdata), size(tsdata))\n\n7×22 Matrix{Float64}:\n  1.26e7      1.23505e7  1.21059e7   1.18662e7  …    8.44577e6    8.2785e6\n  9.03374e-6  4.5798     8.14608    11.2091         70.9046      75.7781\n  1.76877e-5  0.744565   2.53841     4.95787        79.5931      85.8416\n  1.4555e-5   0.0516033  0.363191    1.09095       115.541      129.63\n  1.4e7       1.37228e7  1.34511e7   1.31847e7       9.38448e6    9.19866e6\n -1.19366e-6  0.0101378  0.0700881   0.206669   …   16.4648      18.2247\n -1.69442e-6  0.801628   2.94212     6.17725       221.823      246.265\n\n\n\nplot(abs.(tsdata-noisy_data)')\n\n\n\n\n\n### Neural ODE\n\nann_node = FastChain(FastDense(7, 64, tanh),FastDense(64, 64, tanh), FastDense(64, 64, tanh), FastDense(64, 7))\np = Float64.(initial_params(ann_node))\n\nfunction dudt_node(u,p,t)\n    S,E,I,R,N,D,C = u\n    F,β0,α,κ,μ,σ,γ,d,λ = p_\n    dS,dE,dI,dR,dD = ann_node([S/N,E,I,R,N,D/N,C],p)\n\n    dN = -μ*N # total population\n    dC = σ*E # +cumulative cases\n\n    [dS,dE,dI,dR,dN,dD,dC]\nend\nprob_node = ODEProblem(dudt_node, u0, tspan, p)\ns = concrete_solve(prob_node, Tsit5(), u0, p, saveat = solution.t)\n\nfunction predict(θ)\n    Array(concrete_solve(prob_node, Vern7(), u0, θ, saveat = 1,\n                         abstol=1e-6, reltol=1e-6,\n                         sensealg = SciMLSensitivity.InterpolatingAdjoint(autojacvec=SciMLSensitivity.ReverseDiffVJP())))\nend\n\n# No regularisation right now\nfunction loss(θ)\n    pred = predict(θ)\n    sum(abs2, (noisy_data[2:4,:] .- pred[2:4,:])), pred # + 1e-5*sum(sum.(abs, params(ann)))\nend\n\nloss(p)\n\nconst losses = []\ncallback(θ,l,pred) = begin\n    push!(losses, l)\n    if length(losses)%50==0\n        println(losses[end])\n    end\n    false\nend\n\n┌ Warning: FastChain is being deprecated in favor of Lux.jl. Lux.jl uses functions with explicit parameters f(u,p) like FastChain, but is fully featured and documented machine learning library. See the Lux.jl documentation for more details.\n└ @ DiffEqFlux /Users/stevenchiu/.julia/packages/DiffEqFlux/Em1Aj/src/fast_layers.jl:9\nWARNING: redefinition of constant losses. This may fail, cause incorrect answers, or produce other errors.\n\n\ncallback (generic function with 1 method)\n\n\n\nres1_node = DiffEqFlux.sciml_train(loss, p, ADAM(0.01), cb=callback, maxiters = 500);\n\n┌ Warning: sciml_train is being deprecated in favor of direct usage of Optimization.jl. Please consult the Optimization.jl documentation for more details. Optimization.jl's PolyOpt solver is the polyalgorithm of sciml_train\n└ @ DiffEqFlux /Users/stevenchiu/.julia/packages/DiffEqFlux/Em1Aj/src/train.jl:6\n\n\nLoadError: Need an adjoint for constructor SciMLSensitivity.InterpolatingAdjoint{0, true, Val{:central}, SciMLSensitivity.ReverseDiffVJP{false}}. Gradient is of type ChainRulesCore.ZeroTangent\n\n\n\nres2_node = DiffEqFlux.sciml_train(loss, res1_node.minimizer, BFGS(initial_stepnorm=0.01), cb=callback, maxiters = 10000);\n\n┌ Warning: sciml_train is being deprecated in favor of direct usage of Optimization.jl. Please consult the Optimization.jl documentation for more details. Optimization.jl's PolyOpt solver is the polyalgorithm of sciml_train\n└ @ DiffEqFlux /Users/stevenchiu/.julia/packages/DiffEqFlux/Em1Aj/src/train.jl:6\n\n\nLoadError: Need an adjoint for constructor SciMLSensitivity.InterpolatingAdjoint{0, true, Val{:central}, SciMLSensitivity.ReverseDiffVJP{false}}. Gradient is of type ChainRulesCore.ZeroTangent\n\n\n\nprob_node2 = ODEProblem(dudt_node, u0, tspan, res2_node.minimizer)\ns = solve(prob_node2, Tsit5(), saveat = 1)\n\n┌ Warning: sciml_train is being deprecated in favor of direct usage of Optimization.jl. Please consult the Optimization.jl documentation for more details. Optimization.jl's PolyOpt solver is the polyalgorithm of sciml_train\n└ @ DiffEqFlux /Users/stevenchiu/.julia/packages/DiffEqFlux/Em1Aj/src/train.jl:6\n\n\nLoadError: Need an adjoint for constructor SciMLSensitivity.InterpolatingAdjoint{0, true, Val{:central}, SciMLSensitivity.ReverseDiffVJP{false}}. Gradient is of type ChainRulesCore.ZeroTangent\n\n\n\nscatter(solution, vars=[2,3,4], label=[\"True Exposed\" \"True Infected\" \"True Recovered\"])\nplot!(s, vars=[2,3,4], label=[\"Estimated Exposed\" \"Estimated Infected\" \"Estimated Recovered\"])\n\n# Plot the losses\nplot(losses, yaxis = :log, xaxis = :log, xlabel = \"Iterations\", ylabel = \"Loss\")\n\n# Extrapolate out\nprob_node_extrapolate = ODEProblem(dudt_node,u0, tspan2, res2_node.minimizer)\n_sol_node = solve(prob_node_extrapolate, Vern7(), abstol=1e-12, reltol=1e-12, saveat = 1)\np_node = scatter(solution_extrapolate, vars=[2,3,4], legend = :topleft, label=[\"True Exposed\" \"True Infected\" \"True Recovered\"], title=\"Neural ODE Extrapolation\")\nplot!(p_node,_sol_node, lw=5, vars=[2,3,4], label=[\"Estimated Exposed\" \"Estimated Infected\" \"Estimated Recovered\"])\nplot!(p_node,[20.99,21.01],[0.0,maximum(hcat(Array(solution_extrapolate[2:4,:]),Array(_sol_node[2:4,:])))],lw=5,color=:black,label=\"Training Data End\")\n\nsavefig(\"neuralode_extrapolation.png\")\nsavefig(\"neuralode_extrapolation.pdf\")\n\n\n### Universal ODE Part 1\n\nann = FastChain(FastDense(3, 64, tanh),FastDense(64, 64, tanh), FastDense(64, 1))\np = Float64.(initial_params(ann))\n\nfunction dudt_(u,p,t)\n    S,E,I,R,N,D,C = u\n    F, β0,α,κ,μ,σ,γ,d,λ = p_\n    z = ann([S/N,I,D/N],p) # Exposure does not depend on exposed, removed, or cumulative!\n    dS = -β0*S*F/N - z[1] -μ*S # susceptible\n    dE = β0*S*F/N + z[1] -(σ+μ)*E # exposed\n    dI = σ*E - (γ+μ)*I # infected\n    dR = γ*I - μ*R # removed (recovered + dead)\n    dN = -μ*N # total population\n    dD = d*γ*I - λ*D # severe, critical cases, and deaths\n    dC = σ*E # +cumulative cases\n\n    [dS,dE,dI,dR,dN,dD,dC]\nend\nprob_nn = ODEProblem(dudt_,u0, tspan, p)\ns = concrete_solve(prob_nn, Tsit5(), u0, p, saveat = 1)\n\nplot(solution, vars=[2,3,4])\nplot!(s[2:4,:]')\n\nfunction predict(θ)\n    Array(concrete_solve(prob_nn, Vern7(), u0, θ, saveat = solution.t,\n                         abstol=1e-6, reltol=1e-6,\n                         sensealg = InterpolatingAdjoint(autojacvec=ReverseDiffVJP())))\nend\n\n# No regularisation right now\nfunction loss(θ)\n    pred = predict(θ)\n    sum(abs2, noisy_data[2:4,:] .- pred[2:4,:]), pred # + 1e-5*sum(sum.(abs, params(ann)))\nend\n\nloss(p)\n\nconst losses = []\ncallback(θ,l,pred) = begin\n    push!(losses, l)\n    if length(losses)%50==0\n        println(losses[end])\n    end\n    false\nend\n\nres1_uode = DiffEqFlux.sciml_train(loss, p, ADAM(0.01), cb=callback, maxiters = 500)\nres2_uode = DiffEqFlux.sciml_train(loss, res1_uode.minimizer, BFGS(initial_stepnorm=0.01), cb=callback, maxiters = 10000)\n\nloss(res2_uode.minimizer)\n\nprob_nn2 = ODEProblem(dudt_,u0, tspan, res2_uode.minimizer)\nuode_sol = solve(prob_nn2, Tsit5(), saveat = 1)\nplot(solution, vars=[2,3,4])\nplot!(uode_sol, vars=[2,3,4])\n\n# Plot the losses\nplot(losses, yaxis = :log, xaxis = :log, xlabel = \"Iterations\", ylabel = \"Loss\")\n\n# Collect the state trajectory and the derivatives\nX = noisy_data\n# Ideal derivatives\nDX = Array(solution(solution.t, Val{1}))\n\n# Extrapolate out\nprob_nn2 = ODEProblem(dudt_,u0, tspan2, res2_uode.minimizer)\n_sol_uode = solve(prob_nn2, Vern7(), abstol=1e-12, reltol=1e-12, saveat = 1)\np_uode = scatter(solution_extrapolate, vars=[2,3,4], legend = :topleft, label=[\"True Exposed\" \"True Infected\" \"True Recovered\"], title=\"Universal ODE Extrapolation\")\nplot!(p_uode,_sol_uode, lw = 5, vars=[2,3,4], label=[\"Estimated Exposed\" \"Estimated Infected\" \"Estimated Recovered\"])\nplot!(p_uode,[20.99,21.01],[0.0,maximum(hcat(Array(solution_extrapolate[2:4,:]),Array(_sol_uode[2:4,:])))],lw=5,color=:black,label=\"Training Data End\")\n\nsavefig(\"universalode_extrapolation.png\")\nsavefig(\"universalode_extrapolation.pdf\")\n\n### Universal ODE Part 2: SInDy to Equations\n\n# Create a Basis\n@variables u[1:3]\n# Lots of polynomials\npolys = Operation[]\nfor i ∈ 0:2, j ∈ 0:2, k ∈ 0:2\n    push!(polys, u[1]^i * u[2]^j * u[3]^k)\nend\n\n# And some other stuff\nh = [cos.(u)...; sin.(u)...; unique(polys)...]\nbasis = Basis(h, u)\n\nX = noisy_data\n# Ideal derivatives\nDX = Array(solution(solution.t, Val{1}))\nS,E,I,R,N,D,C = eachrow(X)\nF,β0,α,κ,μ,_,γ,d,λ = p_\nL = β.(0:tspan[end],β0,D,N,κ,α).*S.*I./N\nL̂ = vec(ann([S./N I D./N]',res2_uode.minimizer))\nX̂ = [S./N I D./N]'\n\nscatter(L,title=\"Estimated vs Expected Exposure Term\",label=\"True Exposure\")\nplot!(L̂,label=\"Estimated Exposure\")\nsavefig(\"estimated_exposure.png\")\nsavefig(\"estimated_exposure.pdf\")\n\n# Create an optimizer for the SINDY problem\nopt = SR3()\n# Create the thresholds which should be used in the search process\nthresholds = exp10.(-6:0.1:1)\n\n# Test on original data and without further knowledge\nΨ_direct = SInDy(X[2:4, :], DX[2:4, :], basis, thresholds, opt = opt, maxiter = 50000) # Fail\nprintln(Ψ_direct.basis)\n# Test on ideal derivative data ( not available )\nΨ_ideal = SInDy(X[2:4, 5:end], L[5:end], basis, thresholds, opt = opt, maxiter = 50000) # Succeed\nprintln(Ψ_ideal.basis)\n# Test on uode derivative data\nΨ = SInDy(X̂[:, 2:end], L̂[2:end], basis, thresholds,  opt = opt, maxiter = 10000, normalize = true, denoise = true) # Succeed\nprintln(Ψ.basis)\n\n# Build a ODE for the estimated system\nfunction approx(u,p,t)\n    S,E,I,R,N,D,C = u\n    F, β0,α,κ,μ,σ,γ,d,λ = p_\n    z = Ψ([S/N,I,D/N]) # Exposure does not depend on exposed, removed, or cumulative!\n    dS = -β0*S*F/N - z[1] -μ*S # susceptible\n    dE = β0*S*F/N + z[1] -(σ+μ)*E # exposed\n    dI = σ*E - (γ+μ)*I # infected\n    dR = γ*I - μ*R # removed (recovered + dead)\n    dN = -μ*N # total population\n    dD = d*γ*I - λ*D # severe, critical cases, and deaths\n    dC = σ*E # +cumulative cases\n\n    [dS,dE,dI,dR,dN,dD,dC]\nend\n\n# Create the approximated problem and solution\na_prob = ODEProblem{false}(approx, u0, tspan2, p_)\na_solution = solve(a_prob, Tsit5())\n\np_uodesindy = scatter(solution_extrapolate, vars=[2,3,4], legend = :topleft, label=[\"True Exposed\" \"True Infected\" \"True Recovered\"])\nplot!(p_uodesindy,a_solution, lw = 5, vars=[2,3,4], label=[\"Estimated Exposed\" \"Estimated Infected\" \"Estimated Recovered\"])\nplot!(p_uodesindy,[20.99,21.01],[0.0,maximum(hcat(Array(solution_extrapolate[2:4,:]),Array(_sol_uode[2:4,:])))],lw=5,color=:black,label=\"Training Data End\")\n\nsavefig(\"universalodesindy_extrapolation.png\")\nsavefig(\"universalodesindy_extrapolation.pdf\")\n\n\n\n\n\nReferences\n\n[1] Rackauckas, C. et al. 2020. Universal differential equations for scientific machine learning. arXiv preprint arXiv:2001.04385. (2020)."
  },
  {
    "objectID": "index_julia-intro.html",
    "href": "index_julia-intro.html",
    "title": "PART I: Intro. to Julia",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "resources.html#basics",
    "href": "resources.html#basics",
    "title": "Resources",
    "section": "Basics",
    "text": "Basics\n\nJulia Documentaion (official doc also includes advanced topics)\nThink Julia\nMATLAB-Python-Julia cheatsheet\nJulia cheatsheet\nThe Julia Express"
  },
  {
    "objectID": "resources.html#advanced-topics",
    "href": "resources.html#advanced-topics",
    "title": "Resources",
    "section": "Advanced Topics",
    "text": "Advanced Topics\n\nMIT’s 18.337J/6.338J: Parallel Computing and Scientific Machine Learning course. (videos, book)\nA Deep Introduction to Julia for Data Science and Scientific Computing"
  },
  {
    "objectID": "resources.html#community",
    "href": "resources.html#community",
    "title": "Resources",
    "section": "Community",
    "text": "Community\n\nDiscourse\nForum"
  },
  {
    "objectID": "resources.html#packages",
    "href": "resources.html#packages",
    "title": "Resources",
    "section": "Packages",
    "text": "Packages\n\nSurvey of Julia packages"
  },
  {
    "objectID": "resources.html#news",
    "href": "resources.html#news",
    "title": "Resources",
    "section": "News",
    "text": "News\n\nJulia Computing"
  }
]